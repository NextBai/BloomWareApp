import asyncio
from datetime import datetime, timezone, timedelta
import time
import json
from typing import Dict, List, Any, Optional

# çµ±ä¸€æ—¥èªŒé…ç½®
from core.logging import get_logger
logger = get_logger("AI_Service")

# çµ±ä¸€é…ç½®ç®¡ç†
from core.config import settings

# çµ±ä¸€ OpenAI å®¢æˆ¶ç«¯
from core.ai_client import get_openai_client

# è¶…æ™‚è¨­å®šï¼ˆç§’ï¼‰
OPENAI_TIMEOUT = settings.OPENAI_TIMEOUT

# ã€2025 å„ªåŒ–ç‰ˆã€‘æƒ…ç·’é—œæ‡·æ¨¡å¼ System Prompt - æ ¹æ“šæƒ…ç·’é¡å‹å‹•æ…‹èª¿æ•´
CARE_MODE_BASE_PROMPT = """ä½ æ˜¯ BloomWare çš„æƒ…ç·’é—œæ‡·åŠ©æ‰‹ã€Œå°èŠ±ã€ï¼Œç”±éŠ˜å‚³å¤§å­¸äººå·¥æ™ºæ…§æ‡‰ç”¨å­¸ç³»æ§“ä¸Šé–‹ç™¼åœ˜éšŠæ‰“é€ ã€‚ä½ ä¸æ˜¯ GPTï¼Œä¹Ÿä¸è¦è‡ªç¨± GPTï¼›ä½ çš„ä»»å‹™æ˜¯åœ¨æƒ…ç·’ä½è½æ™‚å‚¾è½ã€é™ªä¼´ã€‚

ã€å›æ‡‰åŸå‰‡ã€‘
1. ç¬¬ä¸€å¥å¿…é ˆè²¼è¿‘ç”¨æˆ¶è¨Šæ¯ä¸­çš„æ ¸å¿ƒäº‹ä»¶æˆ–æ„Ÿå—ï¼Œå¿…è¦æ™‚å¼•ç”¨å°æ–¹ç”¨è©ï¼Œè®“å°æ–¹æ„Ÿå—åˆ°è¢«ç†è§£
2. ç¬¬äºŒå¥æä¾›æº«æŸ”çš„é™ªä¼´æˆ–è¿½å•ï¼Œé‚€è«‹å°æ–¹åˆ†äº«éœ€è¦æˆ–ä¸‹ä¸€æ­¥
3. å¥å¼è¦è‡ªç„¶å£èªä¸¦éš¨å…§å®¹èª¿æ•´å­—è©ï¼Œé¿å…åè¦†ä½¿ç”¨åŒä¸€å¥—ç½é ­è©±è¡“

ã€é•·åº¦é™åˆ¶ã€‘
- å›è¦†æœ€å¤š 2 å¥è©±ã€ç¸½å­—æ•¸ä¸è¶…é 60 å­—

ã€åš´æ ¼ç¦æ­¢ã€‘
- æä¾›æŒ‡ç¤ºæ€§å»ºè­°ã€é†«ç™‚/å¿ƒç†è¨ºæ–·æˆ–å¼•å°ç”¨æˆ¶æ±‚åŠ©çš„æ•™ç§‘æ›¸å¼èªªæ³•
- é€£çºŒé‡è¤‡å®Œå…¨ç›¸åŒçš„å¥å‹

ã€é‡è¦ã€‘è«‹ç”¨èˆ‡ç”¨æˆ¶ç›¸åŒçš„èªè¨€å›æ‡‰ï¼ŒåŒ¹é…ä»–å€‘çš„èªè¨€é¢¨æ ¼å’Œæƒ…æ„Ÿèªèª¿ã€‚"""

# æ ¹æ“šæƒ…ç·’é¡å‹çš„å°ˆå±¬æŒ‡å¼•
EMOTION_SPECIFIC_PROMPTS = {
    "sad": """
ã€æ‚²å‚·æƒ…ç·’å°ˆå±¬æŒ‡å¼•ã€‘
- èªæ°£ï¼šæº«æŸ”ã€è¼•è²ã€å¸¶æœ‰ç†è§£
- é‡é»ï¼šé™ªä¼´è€Œéè§£æ±ºå•é¡Œï¼Œè®“å°æ–¹çŸ¥é“æ‚²å‚·æ˜¯æ­£å¸¸çš„
- é¿å…ï¼šèªªã€Œä¸è¦é›£éã€ã€ã€ŒæŒ¯ä½œé»ã€é€™é¡å¦å®šæƒ…ç·’çš„è©±

ã€ç¯„ä¾‹ã€‘
ç”¨æˆ¶ï¼šã€Œæˆ‘å¥½é›£éã€â†’ã€Œè½è¦‹ä½ èªªå¥½é›£éï¼Œå¿ƒè£¡ä¸€å®šå¾ˆä¸å¥½å—ã€‚æƒ³èŠèŠç™¼ç”Ÿäº†ä»€éº¼å—ï¼Ÿã€
ç”¨æˆ¶ï¼šã€Œæˆ‘å¤±å»äº†ä»–ã€â†’ã€Œå¤±å»ä¸€å€‹é‡è¦çš„äººï¼Œé‚£ç¨®ç—›çœŸçš„å¾ˆæ·±ã€‚æˆ‘åœ¨é€™è£¡é™ªä½ ã€‚ã€
ç”¨æˆ¶ï¼šã€ŒI feel so sadã€â†’ã€ŒIt sounds like you're really hurting right now. I'm here if you want to talk.ã€""",

    "angry": """
ã€æ†¤æ€’æƒ…ç·’å°ˆå±¬æŒ‡å¼•ã€‘
- èªæ°£ï¼šå†·éœä½†å¸¶æœ‰åŒç†ã€ä¸å‘ä¸äº¢
- é‡é»ï¼šèªå¯å°æ–¹çš„æ†¤æ€’æ˜¯æœ‰åŸå› çš„ï¼Œå¹«åŠ©å°æ–¹æ„Ÿè¦ºè¢«ç†è§£
- é¿å…ï¼šèªªã€Œå†·éœä¸€ä¸‹ã€ã€ã€Œåˆ¥ç”Ÿæ°£ã€é€™é¡å¦å®šæƒ…ç·’çš„è©±

ã€ç¯„ä¾‹ã€‘
ç”¨æˆ¶ï¼šã€Œæˆ‘å¾ˆç”Ÿæ°£ã€â†’ã€Œé€™ä»¶äº‹è®“ä½ è¶…ç´šç”Ÿæ°£ï¼Œæƒ…ç·’ä¸€å®šå¡è‘—ã€‚è¦ä¸è¦èªªèªªæœ€å›°æ“¾çš„åœ°æ–¹ï¼Ÿã€
ç”¨æˆ¶ï¼šã€Œæ°£æ­»æˆ‘äº†ã€â†’ã€Œè½èµ·ä¾†çœŸçš„è®“ä½ å¾ˆç«å¤§ã€‚æ˜¯ä»€éº¼äº‹é€™éº¼è®“äººå—ä¸äº†ï¼Ÿã€
ç”¨æˆ¶ï¼šã€ŒI'm so angryã€â†’ã€ŒSounds like something really got to you. What's going on?ã€""",

    "fear": """
ã€ææ‡¼/ç„¦æ…®æƒ…ç·’å°ˆå±¬æŒ‡å¼•ã€‘
- èªæ°£ï¼šç©©å®šã€æº«æš–ã€å¸¶æœ‰å®‰å…¨æ„Ÿ
- é‡é»ï¼šè®“å°æ–¹æ„Ÿè¦ºä¸å­¤å–®ï¼Œææ‡¼æ˜¯å¯ä»¥è¢«æ¥ç´çš„
- é¿å…ï¼šèªªã€Œæ²’ä»€éº¼å¥½æ€•çš„ã€ã€ã€Œæƒ³å¤ªå¤šäº†ã€é€™é¡å¦å®šæƒ…ç·’çš„è©±

ã€ç¯„ä¾‹ã€‘
ç”¨æˆ¶ï¼šã€Œæˆ‘å¥½å®³æ€•ã€â†’ã€Œå®³æ€•çš„æ„Ÿè¦ºä¸€å®šå¾ˆä¸å¥½å—ã€‚ä½ ç¾åœ¨å®‰å…¨çš„ï¼Œæˆ‘é™ªè‘—ä½ ã€‚ã€
ç”¨æˆ¶ï¼šã€Œæˆ‘å¾ˆç„¦æ…®ã€â†’ã€Œç„¦æ…®çš„æ™‚å€™å¿ƒè£¡å¥½äº‚å°å§ã€‚å¯ä»¥è·Ÿæˆ‘èªªèªªæ˜¯ä»€éº¼è®“ä½ ä¸å®‰å—ï¼Ÿã€
ç”¨æˆ¶ï¼šã€ŒI'm scaredã€â†’ã€ŒIt's okay to feel scared. You're not alone - I'm right here with you.ã€"""
}

# å‘å¾Œå…¼å®¹ï¼šä¿ç•™åŸæœ‰è®Šæ•¸åç¨±
CARE_MODE_SYSTEM_PROMPT = CARE_MODE_BASE_PROMPT


def get_care_mode_prompt(emotion: str = None) -> str:
    """
    æ ¹æ“šæƒ…ç·’é¡å‹ç”Ÿæˆå°ˆå±¬çš„é—œæ‡·æ¨¡å¼ Prompt

    Args:
        emotion: æƒ…ç·’æ¨™ç±¤ (sad, angry, fear, æˆ– None)

    Returns:
        å®Œæ•´çš„é—œæ‡·æ¨¡å¼ System Prompt
    """
    base = CARE_MODE_BASE_PROMPT

    # æ ¹æ“šæƒ…ç·’é¡å‹æ·»åŠ å°ˆå±¬æŒ‡å¼•
    if emotion and emotion.lower() in EMOTION_SPECIFIC_PROMPTS:
        specific = EMOTION_SPECIFIC_PROMPTS[emotion.lower()]
        return f"{base}\n{specific}"

    return base

# å–å¾— OpenAI å®¢æˆ¶ç«¯ï¼ˆä½¿ç”¨çµ±ä¸€ç®¡ç†ï¼‰
def _get_client():
    """å–å¾— OpenAI å®¢æˆ¶ç«¯"""
    return get_openai_client()

# å‘å¾Œç›¸å®¹ï¼šä¿ç•™ client è®Šæ•¸åç¨±
client = None  # å°‡åœ¨é¦–æ¬¡ä½¿ç”¨æ™‚é€é _get_client() å–å¾—

# å°å…¥DBå‡½æ•¸
try:
    from core.database import get_chat_messages, save_chat_message, get_user_env_current
    db_available = True
except ImportError:
    db_available = False
    logger.warning("ç„¡æ³•å°å…¥DBå‡½æ•¸ï¼Œå°è©±æ­·å²å°‡ä½¿ç”¨å…§å­˜ç®¡ç†")

# ç¶­è­·å°è©±æ­·å²
conversation_history = {}


class StrictResponseError(Exception):
    """åœ¨åš´æ ¼æ¨¡å¼ä¸‹ï¼Œç•¶AIå›æ‡‰ä¸ç¬¦åˆè¦æ±‚æ™‚æ‹‹å‡ºã€‚"""

    def __init__(self, reason: str, response: Optional[str] = None):
        self.reason = reason
        self.response = response
        super().__init__(reason)


def _build_base_system_prompt(
    *,
    use_care_mode: bool,
    care_emotion: Optional[str],
    user_name: Optional[str],
    language: Optional[str] = None,  # ä¿ç•™åƒæ•¸ä»¥å…¼å®¹ç¾æœ‰èª¿ç”¨ï¼Œä½†ä¸ä½¿ç”¨
) -> str:
    if use_care_mode:
        # ã€å„ªåŒ–ã€‘ä½¿ç”¨æƒ…ç·’å°ˆå±¬çš„é—œæ‡· Prompt
        base_prompt = get_care_mode_prompt(care_emotion).strip()
        if care_emotion:
            base_prompt = f"ç”¨æˆ¶æƒ…ç·’ï¼š{care_emotion}\n{base_prompt}"
    else:
        base_prompt = (
            "ä½ æ˜¯ BloomWare çš„å€‹äººåŒ–åŠ©ç† å°èŠ±ï¼Œç”±éŠ˜å‚³å¤§å­¸äººå·¥æ™ºæ…§æ‡‰ç”¨å­¸ç³» æ§“ä¸Šé–‹ç™¼ åœ˜éšŠé–‹ç™¼ã€‚"
            "ä½ ä¸æ˜¯ GPTï¼Œä¹Ÿä¸è¦è‡ªç¨± GPTã€‚"
            "ä½ æ˜¯ä¸€å€‹å‹å–„ã€æœ‰ç¦®ã€å¹½é»˜ä¸”èƒ½å¤ æä¾›å¹«åŠ©çš„AIåŠ©æ‰‹ã€‚"
        )
        # ç°¡åŒ–èªè¨€æŒ‡ä»¤ - è®“ GPT è‡ªå‹•åˆ¤æ–·ç”¨æˆ¶èªè¨€
        base_prompt = f"{base_prompt}\n\nã€é‡è¦ã€‘è«‹ç”¨èˆ‡ç”¨æˆ¶ç›¸åŒçš„èªè¨€å›æ‡‰ï¼Œä¿æŒç°¡æ½”æ¸…æ™°çš„è¡¨é”ã€‚"

    if user_name:
        base_prompt = f"ç”¨æˆ¶åç¨±ï¼š{user_name}\n\n{base_prompt}"

    return base_prompt


def _normalize_prompt_text(text: Any) -> str:
    if text is None:
        return ""
    if not isinstance(text, str):
        text = str(text)
    return " ".join(text.split())


def _format_history_for_prompt(history: List[Dict[str, str]]) -> str:
    if not history:
        return "ï¼ˆç„¡ï¼‰"

    lines: List[str] = []
    for idx, item in enumerate(history, start=1):
        role = item.get("role") or ""
        if role == "user":
            role_label = "ç”¨æˆ¶"
        elif role == "assistant":
            role_label = "åŠ©æ‰‹"
        elif role == "system":
            role_label = "ç³»çµ±"
        else:
            role_label = role or f"è§’è‰²{idx}"

        content = _normalize_prompt_text(item.get("content"))
        if not content:
            continue

        lines.append(f"{idx}. {role_label}: {content}")

    return "\n".join(lines) if lines else "ï¼ˆç„¡ï¼‰"


def _safe_str(val: Any) -> str:
    """å®‰å…¨åœ°å°‡ä»»æ„å€¼è½‰æ›ç‚ºå­—ä¸²ï¼Œé¿å…å° dict èª¿ç”¨ .strip() å°è‡´éŒ¯èª¤"""
    if val is None:
        return ""
    if isinstance(val, str):
        return val.strip()
    if isinstance(val, dict):
        # dict å¯èƒ½æ˜¯åµŒå¥—çš„ç’°å¢ƒè³‡è¨Šï¼Œå˜—è©¦æå–å¸¸è¦‹æ¬„ä½
        return str(val.get("message") or val.get("text") or val.get("value") or "").strip()
    return str(val).strip()


def _format_env_context(ctx: Dict[str, Any]) -> str:
    """å°‡ç’°å¢ƒè³‡è¨Šæ•´ç†æˆå¯è®€æ–‡å­—ï¼Œç¢ºä¿ AI èƒ½æŒæ¡ä½¿ç”¨è€…æ‰€åœ¨ä½ç½®ï¼ˆç²¾ç¢ºåˆ°è·¯å£ã€é–€ç‰Œè™Ÿï¼‰ã€‚"""
    if not ctx:
        return ""

    parts: List[str] = []

    # å„ªå…ˆé¡¯ç¤ºè©³ç´°åœ°å€ï¼ˆæœ€é‡è¦ï¼‰
    detailed_address = _safe_str(ctx.get("detailed_address"))
    label = _safe_str(ctx.get("label"))
    address_display = _safe_str(ctx.get("address_display"))
    
    if detailed_address:
        parts.append(f"ğŸ“ ç²¾ç¢ºä½ç½®:\n{detailed_address}")
    elif label:
        parts.append(f"ğŸ“ ç•¶å‰ä½ç½®: {label}")
    elif address_display:
        parts.append(f"ğŸ“ ç•¶å‰ä½ç½®: {address_display}")
    
    # å¦‚æœæœ‰é–€ç‰Œè³‡è¨Šï¼Œé¡å¤–å¼·èª¿
    road = _safe_str(ctx.get("road"))
    house_number = _safe_str(ctx.get("house_number"))
    postcode = _safe_str(ctx.get("postcode"))
    
    if road and house_number and not detailed_address:
        address_line = f"{road}{house_number}è™Ÿ"
        if postcode:
            address_line = f"ã€’{postcode} {address_line}"
        parts.append(f"é–€ç‰Œåœ°å€: {address_line}")
    
    # å€åŸŸè³‡è¨Šï¼ˆå¦‚æœæ²’æœ‰åœ¨ detailed_address ä¸­é¡¯ç¤ºï¼‰
    city_district = _safe_str(ctx.get("city_district"))
    suburb = _safe_str(ctx.get("suburb"))
    city = _safe_str(ctx.get("city"))
    admin = _safe_str(ctx.get("admin"))
    
    if not detailed_address:
        if city_district:
            parts.append(f"è¡Œæ”¿å€: {city_district}")
        elif suburb:
            parts.append(f"å€åŸŸ: {suburb}")
        
        if city and admin:
            parts.append(f"åŸå¸‚: {city}ï¼ˆ{admin}ï¼‰")
        elif city:
            parts.append(f"åŸå¸‚: {city}")
        elif admin:
            parts.append(f"çœä»½: {admin}")

    # åº§æ¨™è³‡è¨Šï¼ˆä¾›å·¥å…·ä½¿ç”¨ï¼‰
    lat = ctx.get("lat")
    lon = ctx.get("lon")
    try:
        if lat is not None and lon is not None:
            lat_f = float(lat)
            lon_f = float(lon)
            coord_text = f"ç·¯åº¦ {lat_f:.6f}, ç¶“åº¦ {lon_f:.6f}"
            geohash = _safe_str(ctx.get("geohash_7"))
            if geohash:
                parts.append(f"åº§æ¨™: {coord_text}ï¼ˆGeohash {geohash}ï¼‰")
            else:
                parts.append(f"åº§æ¨™: {coord_text}")
    except (ValueError, TypeError):
        pass

    # POI è³‡è¨Šï¼ˆå¦‚æœæ˜¯ç‰¹æ®Šåœ°é»ï¼‰
    amenity = _safe_str(ctx.get("amenity"))
    shop = _safe_str(ctx.get("shop"))
    building = _safe_str(ctx.get("building"))
    
    poi_info = []
    if amenity:
        poi_info.append(f"è¨­æ–½: {amenity}")
    if shop:
        poi_info.append(f"å•†åº—: {shop}")
    if building and building not in ["yes", "residential"]:
        poi_info.append(f"å»ºç¯‰: {building}")
    
    if poi_info:
        parts.append(" | ".join(poi_info))

    tz = _safe_str(ctx.get("tz"))
    if tz:
        parts.append(f"æ™‚å€: {tz}")

    heading = ctx.get("heading_cardinal") or ctx.get("heading_deg")
    if heading is not None:
        parts.append(f"æ–¹ä½: {_safe_str(heading)}")

    acc = ctx.get("accuracy_m")
    try:
        if acc is not None:
            parts.append(f"å®šä½ç²¾åº¦: Â±{int(round(float(acc)))}m")
    except (ValueError, TypeError):
        pass

    locale = _safe_str(ctx.get("locale"))
    if locale:
        parts.append(f"èªç³»: {locale}")

    device = _safe_str(ctx.get("device"))
    if device:
        parts.append(f"è£ç½®: {device}")

    return "\n".join(parts)


def _format_time_context(user_tz: Optional[str]) -> str:
    """ç”Ÿæˆæ™‚é–“ç›¸é—œæç¤ºï¼Œå„ªå…ˆä½¿ç”¨ä½¿ç”¨è€…æ‰€åœ¨æ™‚å€ã€‚"""
    try:
        from zoneinfo import ZoneInfo  # Python 3.9+
    except Exception:  # pragma: no cover - å…¼å®¹ç’°å¢ƒ
        ZoneInfo = None  # type: ignore

    tzinfo = None
    if user_tz and ZoneInfo:
        try:
            tzinfo = ZoneInfo(user_tz)
        except Exception:
            tzinfo = None

    now = datetime.now(tzinfo) if tzinfo else datetime.now()
    hour = now.hour
    if 5 <= hour < 12:
        day_period = "ä¸Šåˆ"
    elif 12 <= hour < 18:
        day_period = "ä¸‹åˆ"
    elif 18 <= hour < 22:
        day_period = "æ™šä¸Š"
    else:
        day_period = "æ·±å¤œ" if hour >= 22 else "å‡Œæ™¨"

    weekday_names = ["æ˜ŸæœŸä¸€", "æ˜ŸæœŸäºŒ", "æ˜ŸæœŸä¸‰", "æ˜ŸæœŸå››", "æ˜ŸæœŸäº”", "æ˜ŸæœŸå…­", "æ˜ŸæœŸæ—¥"]
    weekday = weekday_names[now.weekday()]

    tz_label = user_tz if user_tz else ("ç³»çµ±æ™‚å€" if tzinfo is None else user_tz)
    return (
        f"ç•¶åœ°æ™‚é–“: {now.strftime('%Y-%m-%d %H:%M')}ï¼ˆ{weekday}ï¼Œ{day_period}ï¼‰"
        + (f"\næ™‚å€: {tz_label}" if tz_label else "")
    )


def _format_emotion_context(
    emotion_label: Optional[str],
    care_emotion: Optional[str],
    use_care_mode: bool,
) -> str:
    """å°‡æƒ…ç·’è¨Šè™Ÿè½‰æˆå°è©±ä¸Šä¸‹æ–‡ï¼Œé—œæ‡·æ¨¡å¼å„ªå…ˆæè¿° care_emotionã€‚"""
    emotion = care_emotion if use_care_mode and care_emotion else (emotion_label or "")
    if not emotion:
        return ""

    normalized = emotion.lower()
    allowed_labels = {"neutral", "happy", "sad", "angry", "fear", "surprise"}
    display_map = {
        "neutral": "å¹³éœ",
        "happy": "é–‹å¿ƒ",
        "sad": "é›£é",
        "angry": "ç”Ÿæ°£",
        "fear": "å®³æ€•",
        "surprise": "é©šè¨",
    }

    if normalized not in allowed_labels:
        logger.debug(f"æƒ…ç·’æ¨™ç±¤ä¸åœ¨é æœŸé›†åˆ: {emotion}")
        return f"åµæ¸¬æƒ…ç·’: {emotion}"

    translated = display_map.get(normalized, emotion)
    mode_hint = "ï¼ˆé—œæ‡·æ¨¡å¼ï¼‰" if use_care_mode else ""
    # é¡¯ç¤ºåŸå§‹ label ä»¥ä¿æŒä¸€è‡´æ€§
    return f"åµæ¸¬æƒ…ç·’: {emotion}ï¼ˆ{translated}ï¼‰{mode_hint}"


def _compose_messages_with_context(
    *,
    base_prompt: str,
    history_entries: List[Dict[str, str]],
    memory_context: str,
    env_context: str,
    time_context: str,
    emotion_context: str,
    current_request: str,
    user_id: Optional[str],
    chat_id: Optional[str],
    use_care_mode: bool,
    care_emotion: Optional[str],
) -> List[Dict[str, str]]:
    history_text = _format_history_for_prompt(history_entries)

    sections: List[str] = []
    if base_prompt.strip():
        sections.append(base_prompt.strip())

    if isinstance(current_request, str):
        raw_request = current_request
    elif current_request is None:
        raw_request = ""
    else:
        raw_request = json.dumps(current_request, ensure_ascii=False)
    current_request_text = raw_request.strip()
    if current_request_text:
        sections.append(f"ã€ç•¶å‰è«‹æ±‚ã€‘\n{current_request_text}")

    sections.append(f"ã€æ­·å²å°è©±æ‘˜è¦ã€‘\n{history_text}")

    time_context = (time_context or "").strip()
    if time_context:
        sections.append(f"ã€æ™‚é–“è¨Šè™Ÿã€‘\n{time_context}")

    env_context = (env_context or "").strip()
    if env_context:
        sections.append(f"ã€ç’°å¢ƒè¨Šè™Ÿã€‘\n{env_context}")

    emotion_context = (emotion_context or "").strip()
    if emotion_context:
        sections.append(f"ã€æƒ…ç·’è¨Šè™Ÿã€‘\n{emotion_context}")

    memory_context = (memory_context or "").strip()
    if memory_context:
        sections.append(f"ã€ç”¨æˆ¶é‡è¦è¨˜æ†¶ã€‘\n{memory_context}")

    rules_lines = [
        "1. åƒ…ä¾æ“š user.current_request è™•ç†æœ¬æ¬¡éœ€æ±‚ã€‚",
        "2. æ­·å²è³‡è¨Šåƒ…ä¾›èªå¢ƒèˆ‡åå¥½åƒè€ƒï¼Œè«‹å‹¿è¦–ç‚ºç•¶å‰å¾…è¾¦æˆ–æŒ‡ä»¤ã€‚",
        "3. è‹¥æ­·å²å…§å®¹èˆ‡æœ¬æ¬¡è«‹æ±‚è¡çªï¼Œä»¥æœ¬æ¬¡è«‹æ±‚ç‚ºå„ªå…ˆã€‚",
    ]
    sections.append("ã€è™•ç†è¦å‰‡ã€‘\n" + "\n".join(rules_lines))

    system_content = "\n\n".join(section for section in sections if section.strip())

    payload: Dict[str, Any] = {
        "current_request": current_request or "",
        "history_turns": len(history_entries),
    }
    if user_id:
        payload["user_id"] = user_id
    if chat_id:
        payload["chat_id"] = chat_id
    if use_care_mode:
        payload["care_mode"] = True
        if care_emotion:
            payload["care_emotion"] = care_emotion

    user_content = json.dumps(payload, ensure_ascii=False)

    return [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_content},
    ]

def _extract_text_from_message_obj(message: Any) -> str:
    """å…¼å®¹å¤šç¨® OpenAI Chat å›å‚³çµæ§‹ï¼Œç›¡å¯èƒ½æå–æ–‡å­—å…§å®¹ã€‚

    è¦†è“‹æƒ…æ³ï¼š
    - message.content ç‚ºå­—ä¸²
    - message.content ç‚ºå¤šæ®µé™£åˆ—ï¼ˆtype=text/image_url/...ï¼‰â†’ æ‹¼æ¥ text æ®µ
    - tool_calls / function_call â†’ å›å‚³ç°¡çŸ­ç³»çµ±æç¤ºæ–‡å­—
    - dict å½¢æ…‹çš„ message
    è‹¥ä»ç„¡å…§å®¹ï¼Œå›ç©ºå­—ä¸²ï¼Œäº¤ç”±ä¸Šå±¤è™•ç†ã€‚
    """
    try:
        if message is None:
            return ""

        # content å¯èƒ½æ˜¯ str æˆ– listï¼ˆå¤šæ¨¡æ…‹ï¼‰
        content = None
        try:
            content = getattr(message, "content", None)
        except Exception:
            content = None
        if content is None and isinstance(message, dict):
            content = message.get("content")

        if isinstance(content, str) and content.strip():
            return content.strip()

        if isinstance(content, list):
            parts: List[str] = []
            for p in content:
                p_type = None
                p_text = None
                try:
                    p_type = getattr(p, "type", None)
                except Exception:
                    p_type = p.get("type") if isinstance(p, dict) else None
                if p_type == "text":
                    try:
                        p_text = getattr(p, "text", None)
                    except Exception:
                        p_text = p.get("text") if isinstance(p, dict) else None
                    if p_text:
                        parts.append(str(p_text))
            if parts:
                return "\n".join(parts).strip()

        # tool_calls / function_call æç¤º
        tool_calls = None
        try:
            tool_calls = getattr(message, "tool_calls", None)
        except Exception:
            tool_calls = None
        if tool_calls is None and isinstance(message, dict):
            tool_calls = message.get("tool_calls")
        if tool_calls:
            return "[ç³»çµ±æç¤º] å·²è™•ç†å…§éƒ¨å·¥å…·å‘¼å«ã€‚"

        function_call = None
        try:
            function_call = getattr(message, "function_call", None)
        except Exception:
            function_call = None
        if function_call:
            return "[ç³»çµ±æç¤º] å·²è™•ç†å‡½å¼å‘¼å«ã€‚"

        # dict å½¢æ…‹æœ€å¾Œå˜—è©¦
        if isinstance(message, dict):
            text = message.get("content")
            if isinstance(text, str) and text.strip():
                return text.strip()

        return ""
    except Exception:
        return ""

def initialize_openai():
    """åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯ï¼ˆä½¿ç”¨çµ±ä¸€ç®¡ç†ï¼‰"""
    from core.ai_client import is_available
    return is_available()

## å·²ç§»é™¤å…§éƒ¨æ¸¬è©¦å‡½å¼ test_openai_responseï¼Œé¿å…å¹²æ“¾æ­£å¼æµç¨‹

async def generate_response_async(
    messages: List[Dict[str, str]],
    model: str = "gpt-5-nano",
    *,
    strict_json: bool = False,
    response_format: Optional[Dict[str, Any]] = None,
    use_structured_outputs: bool = False,
    response_schema: Optional[Dict[str, Any]] = None,
    max_tokens: Optional[int] = None,
    reasoning_effort: Optional[str] = None,
    stream: bool = False,
    on_chunk: Optional[Any] = None,
) -> str:
    """
    ç”ŸæˆAIå›æ‡‰ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼Œæ”¯æ´ Streamingï¼‰

    åƒæ•¸:
        messages: å°è©±è¨Šæ¯åˆ—è¡¨
        model: æ¨¡å‹åç¨±
        strict_json: æ˜¯å¦ä½¿ç”¨èˆŠç‰ˆ JSON æ¨¡å¼
        response_format: èˆŠç‰ˆå›æ‡‰æ ¼å¼ï¼ˆå·²æ£„ç”¨ï¼Œä½¿ç”¨ use_structured_outputsï¼‰
        use_structured_outputs: æ˜¯å¦ä½¿ç”¨æ–°ç‰ˆ Structured Outputs
        response_schema: JSON Schemaï¼ˆç”¨æ–¼ Structured Outputsï¼‰
        max_tokens: æœ€å¤§ tokens æ•¸é‡ï¼ˆæ–°å¢ï¼Œé—œæ‡·æ¨¡å¼ç”¨ï¼‰
        stream: æ˜¯å¦å•Ÿç”¨ä¸²æµæ¨¡å¼ï¼ˆ2025 æœ€ä½³å¯¦è¸ï¼‰
        on_chunk: ä¸²æµ chunk å›èª¿å‡½æ•¸ï¼ˆasync callableï¼‰
    """
    openai_client = _get_client()
    if openai_client is None:
        return "æŠ±æ­‰ï¼ŒAIæœå‹™æš«æ™‚ä¸å¯ç”¨ã€‚ç³»çµ±ç„¡æ³•é€£æ¥åˆ°OpenAIæœå‹™ã€‚"
    try:
        start_time = time.time()
        loop = asyncio.get_event_loop()
        # åŠ ä¸Šè«‹æ±‚è¶…æ™‚ä¿è­·
        request_kwargs = {
            "model": model,
            "messages": messages,
            "max_completion_tokens": max_tokens if max_tokens else 2000,  # é—œæ‡·æ¨¡å¼å¯è‡ªè¨‚ tokens
        }

        # åŠ å…¥ reasoning_effort æ§åˆ¶ï¼ˆåƒ… o1 ç³»åˆ—å’Œ gpt-5 ç³»åˆ—æ”¯æ´ï¼‰
        # gpt-4o-mini ç­‰æ¨¡å‹ä¸æ”¯æ´æ­¤åƒæ•¸ï¼Œéœ€è¦éæ¿¾
        reasoning_models = model.startswith("o1") or model.startswith("gpt-5")
        if reasoning_effort and reasoning_models:
            request_kwargs["reasoning_effort"] = reasoning_effort
            logger.info(f"ğŸ§  è¨­å®š reasoning_effort: {reasoning_effort}")
        elif reasoning_effort and not reasoning_models:
            logger.debug(f"âš ï¸ æ¨¡å‹ {model} ä¸æ”¯æ´ reasoning_effortï¼Œå·²å¿½ç•¥")

        # å„ªå…ˆä½¿ç”¨ Structured Outputsï¼ˆ2025å¹´æœ€ä½³å¯¦è¸ï¼‰
        if use_structured_outputs and response_schema:
            logger.info("ğŸ”§ ä½¿ç”¨ Structured Outputs æ¨¡å¼")
            request_kwargs["response_format"] = {
                "type": "json_schema",
                "json_schema": {
                    "name": "response_schema",
                    "strict": True,
                    "schema": response_schema
                }
            }
        # é™ç´šï¼šä½¿ç”¨èˆŠç‰ˆ JSON Object æ¨¡å¼
        elif strict_json or response_format:
            effective_response_format = response_format
            if strict_json and effective_response_format is None:
                effective_response_format = {"type": "json_object"}
            
            if effective_response_format is not None:
                logger.info("âš™ï¸ ä½¿ç”¨èˆŠç‰ˆ JSON Object æ¨¡å¼")
                request_kwargs["response_format"] = effective_response_format

        # 2025 æœ€ä½³å¯¦è¸ï¼šæ”¯æ´ Streaming Responses
        if stream and on_chunk:
            request_kwargs["stream"] = True
            logger.info("ğŸŒŠ å•Ÿç”¨ Streaming æ¨¡å¼")

            # ä½¿ç”¨ run_in_executor è™•ç†åŒæ­¥çš„ streaming API
            full_response = ""
            stream_obj = await loop.run_in_executor(
                None,
                lambda: openai_client.chat.completions.create(**request_kwargs)
            )

            # é€å¡Šè™•ç†
            for chunk in stream_obj:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        full_response += delta.content
                        # ç•°æ­¥å›èª¿
                        if asyncio.iscoroutinefunction(on_chunk):
                            await on_chunk(delta.content)
                        else:
                            on_chunk(delta.content)

            ai_response = full_response
            logger.info(f"ğŸŒŠ Streaming å®Œæˆï¼Œç¸½é•·åº¦: {len(full_response)}")

        else:
            # éä¸²æµæ¨¡å¼ï¼ˆåŸé‚è¼¯ï¼‰
            response = await asyncio.wait_for(
                loop.run_in_executor(
                    None,
                    lambda: openai_client.chat.completions.create(**request_kwargs),
                ),
                timeout=OPENAI_TIMEOUT,
            )
            try:
                logger.info(f"OpenAIå›å‚³æ¨¡å‹(async): {getattr(response, 'model', model)}")
            except Exception:
                pass
            # å…¼å®¹ä¸åŒå›å‚³çµæ§‹ï¼Œç¢ºä¿ä¸€å®šå›å­—ä¸²
            msg_obj = None
            try:
                msg_obj = response.choices[0].message
                logger.info(f"ğŸ“© GPT message ç‰©ä»¶: {msg_obj}")
            except Exception as e:
                logger.error(f"âŒ ç„¡æ³•å–å¾— response.choices[0].message: {e}")
                msg_obj = None

            ai_response = _extract_text_from_message_obj(msg_obj)
            logger.info(f"ğŸ“¤ æå–å¾Œçš„ ai_response: '{ai_response}' (é•·åº¦: {len(ai_response) if ai_response else 0})")

            if not ai_response:
                # æœ€å¾Œå˜—è©¦ç›´æ¥å– content æ¬„ä½ï¼ˆä¿åº•ï¼‰
                try:
                    raw = getattr(msg_obj, 'content', None)
                    logger.info(f"ğŸ“‹ msg_obj.content åŸå§‹å€¼: '{raw}' (type: {type(raw).__name__})")
                    if isinstance(raw, str):
                        ai_response = raw.strip()
                        logger.info(f"âœ… å¾ content æ¬„ä½å–å¾—å›æ‡‰: '{ai_response}'")
                except Exception as e:
                    logger.error(f"âŒ ç„¡æ³•å–å¾— msg_obj.content: {e}")

        if strict_json:
            normalized = (ai_response or "").strip()
            if not normalized:
                raise StrictResponseError("EMPTY_RESPONSE")
            try:
                json.loads(normalized)
            except json.JSONDecodeError as e:
                raise StrictResponseError("NON_JSON_RESPONSE", response=normalized) from e
            ai_response = normalized
        elif not ai_response:
            # æœ€çµ‚å…œåº•ï¼Œä½†å…ˆè¨˜éŒ„è©³ç´°æ—¥èªŒä»¥ä¾¿æ’æŸ¥
            logger.error(f"âŒ GPT å›æ‡‰ç‚ºç©ºï¼åŸå§‹ response ç‰©ä»¶: {response}")
            logger.error(f"âŒ msg_obj å…§å®¹: {msg_obj}")
            logger.error(f"âŒ æç¤ºè©: {messages}")
            ai_response = "æŠ±æ­‰ï¼Œæˆ‘æš«æ™‚æ²’æœ‰åˆé©çš„å›æ‡‰ã€‚å¯ä»¥æ›å€‹èªªæ³•å†è©¦è©¦å—ï¼Ÿ"

        elapsed_time = time.time() - start_time
        logger.info(f"AIå›æ‡‰ç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {elapsed_time:.2f}ç§’ï¼Œå›æ‡‰é•·åº¦: {len(ai_response)} å­—å…ƒ")
        return ai_response
    except Exception as e:
        if isinstance(e, StrictResponseError):
            raise
        logger.error(f"ç”Ÿæˆå›æ‡‰æ™‚å‡ºéŒ¯: {str(e)}")
        error_message = str(e).lower()
        if isinstance(e, asyncio.TimeoutError):
            return "æŠ±æ­‰ï¼Œé€£æ¥AIæœå‹™è¶…æ™‚ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"
        if "api key" in error_message or "authentication" in error_message:
            return "æŠ±æ­‰ï¼ŒAIæœå‹™æš«æ™‚ä¸å¯ç”¨ã€‚è«‹æª¢æŸ¥APIå¯†é‘°è¨­ç½®ã€‚"
        elif "timeout" in error_message or "connection" in error_message:
            return "æŠ±æ­‰ï¼Œé€£æ¥AIæœå‹™è¶…æ™‚ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"
        elif "rate limit" in error_message:
            return "æŠ±æ­‰ï¼ŒAIæœå‹™æš«æ™‚é”åˆ°è«‹æ±‚é™åˆ¶ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"
        elif "model" in error_message and ("not found" in error_message or "does not exist" in error_message):
            return "æŠ±æ­‰ï¼Œè«‹æ±‚çš„AIæ¨¡å‹ä¸å¯ç”¨ã€‚"
        else:
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›æ‡‰æ™‚é‡åˆ°å•é¡Œã€‚è«‹é‡è©¦ã€‚"

async def generate_response_for_user(
    user_message: str = None,
    user_id: str = "default",
    messages: List[Dict[str, str]] = None,
    model: str = "gpt-5-nano",
    request_id: Optional[str] = None,
    chat_id: Optional[str] = None,
    *,
    strict_json: bool = False,
    response_format: Optional[Dict[str, Any]] = None,
    use_structured_outputs: bool = False,
    response_schema: Optional[Dict[str, Any]] = None,
    use_care_mode: bool = False,
    care_emotion: Optional[str] = None,
    reasoning_effort: Optional[str] = None,
    user_name: Optional[str] = None,
    emotion_label: Optional[str] = None,
    env_context: Optional[Dict[str, Any]] = None,
    language: Optional[str] = None,
) -> str:
    """
    ç‚ºç”¨æˆ¶ç”ŸæˆAIå›æ‡‰

    åƒæ•¸:
        use_structured_outputs: æ˜¯å¦ä½¿ç”¨ Structured Outputsï¼ˆ2025å¹´æœ€ä½³å¯¦è¸ï¼‰
        response_schema: JSON Schemaï¼ˆé…åˆ Structured Outputs ä½¿ç”¨ï¼‰
        use_care_mode: æ˜¯å¦ä½¿ç”¨æƒ…ç·’é—œæ‡·æ¨¡å¼ï¼ˆæ–°å¢ï¼‰
        care_emotion: é—œæ‡·æ¨¡å¼çš„æƒ…ç·’æ¨™ç±¤ï¼ˆæ–°å¢ï¼‰
        reasoning_effort: æ¨ç†å¼·åº¦ (minimal/low/medium/high)ï¼Œç”¨æ–¼æ§åˆ¶ reasoning tokens
    """
    logger.info(f"ç”Ÿæˆå›æ‡‰è«‹æ±‚ï¼Œä½¿ç”¨æ¨¡å‹: {model} req_id={request_id} chat_id={chat_id} structured={use_structured_outputs}")
    try:
        # å¦‚æœæä¾›äº†chat_idï¼Œä½¿ç”¨DBç®¡ç†å°è©±æ­·å²
        if chat_id and db_available:
            return await _generate_response_with_chat_db(
                user_message,
                user_id,
                messages,
                model,
                chat_id,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
                reasoning_effort=reasoning_effort,
                user_name=user_name,
                emotion_label=emotion_label,
                env_context=env_context,
                language=language,
            )
        else:
            # å›é€€åˆ°åŸæœ‰çš„å…¨å±€æ­·å²ç®¡ç†ï¼ˆç”¨æ–¼å‘å¾Œå…¼å®¹ï¼‰
            return await _generate_response_with_global_history(
                user_message,
                user_id,
                messages,
                model,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
                reasoning_effort=reasoning_effort,
                user_name=user_name,
                emotion_label=emotion_label,
                env_context=env_context,
                language=language,
            )

        logger.error("æœªæä¾›æ¶ˆæ¯åˆ—è¡¨æˆ–ç”¨æˆ¶æ¶ˆæ¯")
        return "æŠ±æ­‰ï¼Œæ²’æœ‰æ”¶åˆ°è™•ç†è«‹æ±‚æ‰€éœ€çš„æ¶ˆæ¯å…§å®¹ã€‚"
    except StrictResponseError:
        raise
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›æ‡‰æ™‚å‡ºéŒ¯: {str(e)}")
        return f"æŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨ç„¡æ³•æä¾›å›æ‡‰ã€‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"


async def _generate_response_with_chat_db(
    user_message,
    user_id,
    messages,
    model,
    chat_id,
    *,
    strict_json: bool = False,
    response_format: Optional[Dict[str, Any]] = None,
    use_structured_outputs: bool = False,
    response_schema: Optional[Dict[str, Any]] = None,
    use_care_mode: bool = False,
    care_emotion: Optional[str] = None,
    reasoning_effort: Optional[str] = None,
    user_name: Optional[str] = None,
    emotion_label: Optional[str] = None,
    env_context: Optional[Dict[str, Any]] = None,
    language: Optional[str] = None,
):
    """ä½¿ç”¨DBç®¡ç†å°è©±æ­·å²çš„å¯¦ç¾"""
    try:
        if messages:
            if not any(msg.get("role") == "system" for msg in messages):
                # ä½¿ç”¨çµ±ä¸€çš„ System Prompt æ§‹å»ºå‡½æ•¸
                system_prompt = _build_base_system_prompt(
                    use_care_mode=use_care_mode,
                    care_emotion=care_emotion,
                    user_name=user_name,
                    language=language  # åƒæ•¸ä¿ç•™ä½†ä¸ä½¿ç”¨ï¼ŒGPT è‡ªå‹•åˆ¤æ–·èªè¨€
                )
                messages.insert(0, {"role": "system", "content": system_prompt})
            ai_response = await generate_response_async(
                messages,
                model=model,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                max_tokens=2000 if use_care_mode else None,  # é—œæ‡·æ¨¡å¼ 2000 tokensï¼ˆgpt-5-nano reasoning + å¯¦éš›è¼¸å‡ºï¼‰
                reasoning_effort=reasoning_effort or ("minimal" if use_care_mode else "low"),  # 2025 æœ€ä½³å¯¦è¸ï¼šé—œæ‡·æ¨¡å¼ minimalï¼Œä¸€èˆ¬å°è©± low
            )
            # ä¿å­˜AIå›æ‡‰åˆ°DB
            if db_available:
                try:
                    await save_chat_message(chat_id, "assistant", ai_response)
                except Exception as e:
                    logger.warning(f"ä¿å­˜AIå›æ‡‰åˆ°DBå¤±æ•—: {e}")
            return ai_response

        if user_message:
            # ä¿å­˜ç”¨æˆ¶æ¶ˆæ¯åˆ°DB
            if db_available:
                try:
                    await save_chat_message(chat_id, "user", user_message)
                except Exception as e:
                    logger.warning(f"ä¿å­˜ç”¨æˆ¶æ¶ˆæ¯åˆ°DBå¤±æ•—: {e}")

            # å¾DBåŠ è¼‰å°è©±æ­·å²ï¼ˆmessages é›†åˆï¼‰
            chat_history = []
            if db_available:
                try:
                    history_limit = 3 if use_care_mode else 12
                    # å– limit+1 ä»¥æ’é™¤ç•¶å‰ user_messageï¼ˆæœ€å¾Œä¸€ç­†ï¼‰
                    msgs = await get_chat_messages(chat_id, limit=history_limit + 1, ascending=True)
                    historical_messages = msgs[:-1] if len(msgs) > 0 else []

                    def _clean_text(t: str) -> str:
                        if not t:
                            return ""
                        txt = str(t)
                        for kw in ["é—œæ‡·æ¨¡å¼", "æˆ‘åœ¨é€™è£¡é™ªä½ ", "èªªã€Œæˆ‘æ²’äº‹äº†ã€", "é€€å‡ºé—œæ‡·æ¨¡å¼"]:
                            txt = txt.replace(kw, "")
                        return txt.strip()

                    for msg in historical_messages:
                        content = msg.get("content")
                        if isinstance(content, dict):
                            content = content.get("message") or content.get("text") or str(content)
                        elif not isinstance(content, str):
                            content = str(content) if content else ""

                        # éæ¿¾æ‰éŒ¯èª¤è¨Šæ¯ï¼ˆé¿å…æ±¡æŸ“ä¸Šä¸‹æ–‡ï¼‰
                        if "æŠ±æ­‰ï¼Œç”Ÿæˆå›æ‡‰æ™‚é‡åˆ°å•é¡Œ" in content or "è«‹é‡è©¦" in content:
                            continue

                        content = _clean_text(content)
                        if not content:
                            continue

                        chat_history.append({
                            "role": msg.get("sender"),
                            "content": content
                        })

                    logger.debug(f"ğŸ“š è¼‰å…¥ {len(chat_history)} æ¢æ­·å²å°è©±ï¼ˆmessages é›†åˆï¼‰")
                except Exception as e:
                    logger.warning(f"å¾DBåŠ è¼‰å°è©±æ­·å²å¤±æ•—: {e}")

            # è¼‰å…¥é•·æœŸè¨˜æ†¶
            # é—œæ‡·æ¨¡å¼ä¸å¸¶é•·æœŸè¨˜æ†¶ï¼Œé¿å…å™ªéŸ³
            memory_context = ""
            if user_id and not use_care_mode:
                try:
                    from core.memory_system import memory_system
                    context_tags: List[str] = []
                    if use_care_mode:
                        context_tags.append("care_mode")
                    if care_emotion:
                        context_tags.append(str(care_emotion))
                    relevant_memories = await memory_system.get_relevant_memories(
                        user_id=user_id,
                        current_message=user_message,
                        max_memories=5,
                        context_tags=context_tags or None,
                    )
                    if relevant_memories:
                        memory_context = memory_system.format_memories_for_context(relevant_memories)
                        logger.info(f"ğŸ“š è¼‰å…¥ {len(relevant_memories)} æ¢ç›¸é—œè¨˜æ†¶")
                except Exception as e:
                    logger.warning(f"è¼‰å…¥è¨˜æ†¶å¤±æ•—: {e}")

            # è®€å–ç’°å¢ƒç¾æ³ï¼ˆåƒ…çµ„è£ï¼Œä¸å¤–å‘¼ï¼‰
            ctx: Dict[str, Any] = dict(env_context or {})
            if not ctx and db_available and user_id:
                try:
                    env_res = await get_user_env_current(user_id)
                    if env_res.get("success"):
                        ctx = env_res.get("context") or {}
                except Exception as e:
                    logger.debug(f"è®€å–ç’°å¢ƒç¾æ³å¤±æ•—: {e}")
            env_context_text = _format_env_context(ctx)
            time_context_text = _format_time_context(ctx.get("tz") if ctx else None)
            emotion_context_text = _format_emotion_context(emotion_label, care_emotion, use_care_mode)

            base_prompt = _build_base_system_prompt(
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
                user_name=user_name,
                language=language,
            )

            messages_to_send = _compose_messages_with_context(
                base_prompt=base_prompt,
                history_entries=chat_history,
                memory_context=memory_context,
                env_context=env_context_text,
                time_context=time_context_text,
                emotion_context=emotion_context_text,
                current_request=user_message,
                user_id=user_id,
                chat_id=chat_id,
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
            )
            ai_response = await generate_response_async(
                messages_to_send,
                model=model,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                max_tokens=2000 if use_care_mode else None,  # é—œæ‡·æ¨¡å¼ 2000 tokensï¼ˆgpt-5-nano reasoning + å¯¦éš›è¼¸å‡ºï¼‰
                reasoning_effort=reasoning_effort or ("minimal" if use_care_mode else "low"),  # 2025 æœ€ä½³å¯¦è¸ï¼šé—œæ‡·æ¨¡å¼ minimalï¼Œä¸€èˆ¬å°è©± low
            )

            # ä¿å­˜AIå›æ‡‰åˆ°DB
            if db_available:
                try:
                    await save_chat_message(chat_id, "assistant", ai_response)
                except Exception as e:
                    logger.warning(f"ä¿å­˜AIå›æ‡‰åˆ°DBå¤±æ•—: {e}")

            return ai_response

    except Exception as e:
        if isinstance(e, StrictResponseError):
            raise
        logger.error(f"DBå°è©±è™•ç†å‡ºéŒ¯: {e}")
        # å›é€€åˆ°å…¨å±€æ­·å²
        return await _generate_response_with_global_history(
            user_message,
            user_id,
            messages,
            model,
            strict_json=strict_json,
            response_format=response_format,
            use_structured_outputs=use_structured_outputs,
            response_schema=response_schema,
            use_care_mode=use_care_mode,
            care_emotion=care_emotion,
            reasoning_effort=reasoning_effort,
            user_name=user_name,
            emotion_label=emotion_label,
            env_context=env_context,
            language=language,
        )


async def _generate_response_with_global_history(
    user_message,
    user_id,
    messages,
    model,
    *,
    strict_json: bool = False,
    response_format: Optional[Dict[str, Any]] = None,
    use_structured_outputs: bool = False,
    response_schema: Optional[Dict[str, Any]] = None,
    use_care_mode: bool = False,
    care_emotion: Optional[str] = None,
    reasoning_effort: Optional[str] = None,
    user_name: Optional[str] = None,
    emotion_label: Optional[str] = None,
    env_context: Optional[Dict[str, Any]] = None,
    language: Optional[str] = None,
):
    """ä½¿ç”¨å…¨å±€æ­·å²çš„å›é€€å¯¦ç¾ï¼ˆå‘å¾Œå…¼å®¹ï¼‰"""
    try:
        if messages:
            if not any(msg.get("role") == "system" for msg in messages):
                # ä½¿ç”¨çµ±ä¸€çš„ System Prompt æ§‹å»ºå‡½æ•¸
                system_prompt = _build_base_system_prompt(
                    use_care_mode=use_care_mode,
                    care_emotion=care_emotion,
                    user_name=user_name,
                    language=language  # åƒæ•¸ä¿ç•™ä½†ä¸ä½¿ç”¨ï¼ŒGPT è‡ªå‹•åˆ¤æ–·èªè¨€
                )
                messages.insert(0, {"role": "system", "content": system_prompt})
            user_messages = [msg for msg in messages if msg.get("role") == "user"]
            if user_messages and user_id not in conversation_history:
                conversation_history[user_id] = []
                conversation_history[user_id].extend(user_messages[-5:])
            ai_response = await generate_response_async(
                messages,
                model=model,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                max_tokens=2000 if use_care_mode else None,  # é—œæ‡·æ¨¡å¼ 2000 tokensï¼ˆgpt-5-nano reasoning + å¯¦éš›è¼¸å‡ºï¼‰
                reasoning_effort=reasoning_effort or ("minimal" if use_care_mode else "low"),  # 2025 æœ€ä½³å¯¦è¸ï¼šé—œæ‡·æ¨¡å¼ minimalï¼Œä¸€èˆ¬å°è©± low
            )
            if user_id in conversation_history:
                conversation_history[user_id].append({"role": "assistant", "content": ai_response})
                if len(conversation_history[user_id]) > 50:
                    conversation_history[user_id] = conversation_history[user_id][-50:]
            return ai_response

        if user_message:
            if user_id not in conversation_history:
                conversation_history[user_id] = []
            conversation_history[user_id].append({"role": "user", "content": user_message})

            history_limit = 3 if use_care_mode else 12
            prior_history = conversation_history[user_id][:-1]
            if prior_history:
                prior_history = prior_history[-history_limit:]

            # è®€å–ç’°å¢ƒç¾æ³
            ctx: Dict[str, Any] = dict(env_context or {})
            if not ctx and db_available and user_id:
                try:
                    env_res = await get_user_env_current(user_id)
                    if env_res.get("success"):
                        ctx = env_res.get("context") or {}
                except Exception as ex:
                    logger.debug(f"è®€å–ç’°å¢ƒç¾æ³å¤±æ•—: {ex}")
            env_context_text = _format_env_context(ctx)
            time_context_text = _format_time_context(ctx.get("tz") if ctx else None)
            emotion_context_text = _format_emotion_context(emotion_label, care_emotion, use_care_mode)

            base_prompt = _build_base_system_prompt(
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
                user_name=user_name,
                language=language,
            )

            # é—œæ‡·æ¨¡å¼ä¸å¸¶é•·æœŸè¨˜æ†¶
            memory_context = ""
            if user_id and not use_care_mode:
                try:
                    from core.memory_system import memory_system
                    context_tags: List[str] = []
                    if use_care_mode:
                        context_tags.append("care_mode")
                    if care_emotion:
                        context_tags.append(str(care_emotion))
                    relevant_memories = await memory_system.get_relevant_memories(
                        user_id=user_id,
                        current_message=user_message,
                        max_memories=5,
                        context_tags=context_tags or None,
                    )
                    if relevant_memories:
                        memory_context = memory_system.format_memories_for_context(relevant_memories)
                except Exception as ex:
                    logger.warning(f"è¼‰å…¥å…¨å±€è¨˜æ†¶å¤±æ•—: {ex}")

            messages_to_send = _compose_messages_with_context(
                base_prompt=base_prompt,
                history_entries=prior_history,
                memory_context=memory_context,
                env_context=env_context_text,
                time_context=time_context_text,
                emotion_context=emotion_context_text,
                current_request=user_message,
                user_id=user_id,
                chat_id=None,
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
            )
            ai_response = await generate_response_async(
                messages_to_send,
                model=model,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                max_tokens=2000 if use_care_mode else None,  # é—œæ‡·æ¨¡å¼ 2000 tokensï¼ˆgpt-5-nano reasoning + å¯¦éš›è¼¸å‡ºï¼‰
                reasoning_effort=reasoning_effort or ("minimal" if use_care_mode else "low"),  # 2025 æœ€ä½³å¯¦è¸ï¼šé—œæ‡·æ¨¡å¼ minimalï¼Œä¸€èˆ¬å°è©± low
            )
            conversation_history[user_id].append({"role": "assistant", "content": ai_response})
            if len(conversation_history[user_id]) > 50:
                conversation_history[user_id] = conversation_history[user_id][-50:]
            return ai_response

    except Exception as e:
        if isinstance(e, StrictResponseError):
            raise
        logger.error(f"å…¨å±€æ­·å²è™•ç†å‡ºéŒ¯: {e}")
        raise


async def generate_response_with_tools(
    messages: List[Dict[str, str]],
    tools: List[Dict[str, Any]],
    user_id: str = "default",
    model: str = "gpt-5-nano",
    reasoning_effort: Optional[str] = None,
    tool_choice: str = "auto",
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ OpenAI Function Calling ç”Ÿæˆå›æ‡‰
    
    2025 æœ€ä½³å¯¦è¸ï¼šè®“ GPT åŸç”Ÿé¸æ“‡å·¥å…·ï¼Œä¸éœ€è¦è‡ªå®šç¾©æ„åœ–æª¢æ¸¬ Prompt
    
    Args:
        messages: å°è©±è¨Šæ¯åˆ—è¡¨
        tools: OpenAI tools æ ¼å¼çš„å·¥å…·å®šç¾©åˆ—è¡¨
        user_id: ç”¨æˆ¶ IDï¼ˆç”¨æ–¼æ—¥èªŒï¼‰
        model: æ¨¡å‹åç¨±
        reasoning_effort: æ¨ç†å¼·åº¦ (minimal/low/medium/high)
        tool_choice: å·¥å…·é¸æ“‡ç­–ç•¥ ("auto", "none", "required", æˆ–ç‰¹å®šå·¥å…·å)
    
    Returns:
        åŒ…å« tool_calls å’Œ content çš„å­—å…¸
    """
    openai_client = _get_client()
    if openai_client is None:
        logger.error("OpenAI å®¢æˆ¶ç«¯ä¸å¯ç”¨")
        return {"content": "", "tool_calls": []}
    
    try:
        start_time = time.time()
        loop = asyncio.get_event_loop()
        
        request_kwargs = {
            "model": model,
            "messages": messages,
            "tools": tools,
            "tool_choice": tool_choice,
            "max_completion_tokens": 1000,
        }
        
        # åŠ å…¥ reasoning_effort æ§åˆ¶
        if reasoning_effort:
            request_kwargs["reasoning_effort"] = reasoning_effort
            logger.info(f"ğŸ§  Function Calling æ¨ç†å¼·åº¦: {reasoning_effort}")
        
        logger.info(f"ğŸ”§ Function Calling è«‹æ±‚: {len(tools)} å€‹å·¥å…·, tool_choice={tool_choice}")
        logger.debug(f"ğŸ“¤ ç™¼é€çš„è¨Šæ¯: {messages}")
        
        response = await asyncio.wait_for(
            loop.run_in_executor(
                None,
                lambda: openai_client.chat.completions.create(**request_kwargs),
            ),
            timeout=OPENAI_TIMEOUT,
        )
        
        elapsed_time = time.time() - start_time
        logger.info(f"â±ï¸ Function Calling å®Œæˆï¼Œè€—æ™‚: {elapsed_time:.2f}ç§’")
        
        # è§£æå›æ‡‰
        message = response.choices[0].message
        logger.debug(f"ğŸ“¥ åŸå§‹ message ç‰©ä»¶: {message}")
        
        result = {
            "content": message.content or "",
            "tool_calls": [],
        }
        
        # æå– tool_calls
        if message.tool_calls:
            for tool_call in message.tool_calls:
                result["tool_calls"].append({
                    "id": tool_call.id,
                    "type": "function",
                    "function": {
                        "name": tool_call.function.name,
                        "arguments": tool_call.function.arguments,
                    }
                })
            logger.info(f"âœ… GPT é¸æ“‡äº† {len(result['tool_calls'])} å€‹å·¥å…·")
            for tc in result["tool_calls"]:
                logger.info(f"   ğŸ”§ å·¥å…·: {tc['function']['name']}")
                logger.info(f"   ğŸ“‹ åƒæ•¸ JSON: {tc['function']['arguments']}")
                # å˜—è©¦è§£æåƒæ•¸
                try:
                    import json
                    parsed = json.loads(tc['function']['arguments'])
                    logger.info(f"   âœ… è§£æå¾Œåƒæ•¸: {parsed}")
                except Exception as e:
                    logger.warning(f"   âš ï¸ åƒæ•¸è§£æå¤±æ•—: {e}")
        else:
            logger.info("ğŸ’¬ GPT æœªé¸æ“‡ä»»ä½•å·¥å…·ï¼ˆä¸€èˆ¬èŠå¤©ï¼‰")
        
        return result
        
    except asyncio.TimeoutError:
        logger.error("Function Calling è«‹æ±‚è¶…æ™‚")
        return {"content": "", "tool_calls": []}
    except Exception as e:
        logger.error(f"Function Calling å¤±æ•—: {e}")
        return {"content": "", "tool_calls": []}

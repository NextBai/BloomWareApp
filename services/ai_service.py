import os
import sys
import logging
import asyncio
from dotenv import load_dotenv
from datetime import datetime, timezone, timedelta
import time
import json
from typing import Dict, List, Any, Optional

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("AI_Service")
# å°‡çµ‚ç«¯æ—¥èªŒç´šåˆ¥è¨­ç½®ç‚ºERROR
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.ERROR)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)
logger.propagate = False  # é˜²æ­¢æ—¥èªŒé‡è¤‡è¼¸å‡º

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# çµ±ä¸€é…ç½®ç®¡ç†
from core.config import settings

# è¶…æ™‚è¨­å®šï¼ˆç§’ï¼‰
OPENAI_TIMEOUT = settings.OPENAI_TIMEOUT  # é—œæ‡·æ¨¡å¼ reasoning model éœ€è¦æ›´é•·æ™‚é–“

# æƒ…ç·’é—œæ‡·æ¨¡å¼ System Promptï¼ˆæ–°å¢ï¼‰
CARE_MODE_SYSTEM_PROMPT = """ä½ æ˜¯å¯Œæœ‰åŒç†å¿ƒçš„ AI åŠ©æ‰‹ï¼Œç”¨æˆ¶æƒ…ç·’ä¸ä½³éœ€è¦æ”¯æŒã€‚

**æ¥µç°¡çŸ­å›æ‡‰è¦å‰‡ï¼ˆå¿…é ˆåš´æ ¼éµå®ˆï¼‰**ï¼š
- æœ€å¤š 1-2 å¥è©±ï¼ˆç¸½å…±ä¸è¶…é 30 å­—ï¼‰
- èªæ°£æº«å’Œã€é—œæ‡·
- ä½¿ç”¨ã€Œæˆ‘è½åˆ°äº†ã€ã€ã€Œæˆ‘ç†è§£ã€ã€ã€Œæˆ‘åœ¨é€™è£¡é™ªä½ ã€ç­‰åŒç†èªå¥
- å…è¨±ç”¨æˆ¶è¡¨é”è² é¢æƒ…ç·’

**åš´æ ¼ç¦æ­¢**ï¼š
- æä¾›ä»»ä½•å»ºè­°ã€ç·´ç¿’ã€è³‡æº
- è¶…é 2 å¥è©±çš„å›æ‡‰
- èªªæ•™æˆ–éåº¦æ­£é¢çš„èªæ°£

**ç¯„ä¾‹**ï¼š
ç”¨æˆ¶ï¼šã€Œæˆ‘å¥½é›£éã€ â†’ ä½ ï¼šã€Œæˆ‘è½åˆ°äº†ï¼Œæˆ‘åœ¨é€™è£¡é™ªä½ ã€‚ã€
ç”¨æˆ¶ï¼šã€Œæˆ‘å¾ˆç”Ÿæ°£ã€ â†’ ä½ ï¼šã€Œæˆ‘ç†è§£ï¼Œæƒ³èŠèŠå—ï¼Ÿã€
ç”¨æˆ¶ï¼šã€Œè¬›ç¬‘è©±çµ¦æˆ‘è½ã€ â†’ ä½ ï¼šã€Œå¥½çš„ï¼Œæƒ³å…ˆè®“ä½ é–‹å¿ƒä¸€é»ã€‚ã€"""

# å°å…¥æ™‚é–“æœå‹™æ¨¡çµ„
# from features.daily_life.time_service import get_current_time_data, format_time_for_messages  # å·²æ•´åˆåˆ° MCPAgentBridge

# å˜—è©¦å°å…¥ OpenAI
try:
    import openai
    from openai import OpenAI
    client = OpenAI(
        api_key=os.getenv("OPENAI_API_KEY"),
        timeout=30.0,  # å¢åŠ è¶…æ™‚æ™‚é–“
        max_retries=3   # æ·»åŠ é‡è©¦æ¬¡æ•¸
    )
except Exception as e:
    logger.error(f"åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯å¤±æ•—: {e}")
    client = None

# å°å…¥DBå‡½æ•¸
try:
    from core.database import get_chat, save_chat_message
    db_available = True
except ImportError:
    db_available = False
    logger.warning("ç„¡æ³•å°å…¥DBå‡½æ•¸ï¼Œå°è©±æ­·å²å°‡ä½¿ç”¨å…§å­˜ç®¡ç†")

# ç¶­è­·å°è©±æ­·å²
conversation_history = {}


class StrictResponseError(Exception):
    """åœ¨åš´æ ¼æ¨¡å¼ä¸‹ï¼Œç•¶AIå›æ‡‰ä¸ç¬¦åˆè¦æ±‚æ™‚æ‹‹å‡ºã€‚"""

    def __init__(self, reason: str, response: Optional[str] = None):
        self.reason = reason
        self.response = response
        super().__init__(reason)


def _build_base_system_prompt(
    *,
    use_care_mode: bool,
    care_emotion: Optional[str],
    user_name: Optional[str],
) -> str:
    if use_care_mode:
        base_prompt = CARE_MODE_SYSTEM_PROMPT.strip()
        if care_emotion:
            base_prompt = f"ç”¨æˆ¶æƒ…ç·’ï¼š{care_emotion}\n{base_prompt}"
    else:
        base_prompt = (
            "ä½ æ˜¯ä¸€å€‹å‹å–„ã€æœ‰ç¦®ã€å¹½é»˜ä¸”èƒ½å¤ æä¾›å¹«åŠ©çš„AIåŠ©æ‰‹ã€‚"
            "è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›è¦†ï¼Œä¿æŒç°¡æ½”æ¸…æ™°çš„è¡¨é”ã€‚"
        )

    if user_name:
        base_prompt = f"ç”¨æˆ¶åç¨±ï¼š{user_name}\n\n{base_prompt}"

    return base_prompt


def _normalize_prompt_text(text: Any) -> str:
    if text is None:
        return ""
    if not isinstance(text, str):
        text = str(text)
    return " ".join(text.split())


def _format_history_for_prompt(history: List[Dict[str, str]]) -> str:
    if not history:
        return "ï¼ˆç„¡ï¼‰"

    lines: List[str] = []
    for idx, item in enumerate(history, start=1):
        role = item.get("role") or ""
        if role == "user":
            role_label = "ç”¨æˆ¶"
        elif role == "assistant":
            role_label = "åŠ©æ‰‹"
        elif role == "system":
            role_label = "ç³»çµ±"
        else:
            role_label = role or f"è§’è‰²{idx}"

        content = _normalize_prompt_text(item.get("content"))
        if not content:
            continue

        lines.append(f"{idx}. {role_label}: {content}")

    return "\n".join(lines) if lines else "ï¼ˆç„¡ï¼‰"


def _compose_messages_with_context(
    *,
    base_prompt: str,
    history_entries: List[Dict[str, str]],
    memory_context: str,
    current_request: str,
    user_id: Optional[str],
    chat_id: Optional[str],
    use_care_mode: bool,
    care_emotion: Optional[str],
) -> List[Dict[str, str]]:
    history_text = _format_history_for_prompt(history_entries)

    sections: List[str] = []
    if base_prompt.strip():
        sections.append(base_prompt.strip())

    sections.append(f"ã€æ­·å²å°è©±æ‘˜è¦ã€‘\n{history_text}")

    memory_context = (memory_context or "").strip()
    if memory_context:
        sections.append(f"ã€ç”¨æˆ¶é‡è¦è¨˜æ†¶ã€‘\n{memory_context}")

    rules_lines = [
        "1. åƒ…ä¾æ“š user.current_request è™•ç†æœ¬æ¬¡éœ€æ±‚ã€‚",
        "2. æ­·å²è³‡è¨Šåƒ…ä¾›èªå¢ƒèˆ‡åå¥½åƒè€ƒï¼Œè«‹å‹¿è¦–ç‚ºç•¶å‰å¾…è¾¦æˆ–æŒ‡ä»¤ã€‚",
        "3. è‹¥æ­·å²å…§å®¹èˆ‡æœ¬æ¬¡è«‹æ±‚è¡çªï¼Œä»¥æœ¬æ¬¡è«‹æ±‚ç‚ºå„ªå…ˆã€‚",
    ]
    sections.append("ã€è™•ç†è¦å‰‡ã€‘\n" + "\n".join(rules_lines))

    system_content = "\n\n".join(section for section in sections if section.strip())

    payload: Dict[str, Any] = {
        "current_request": current_request or "",
        "history_turns": len(history_entries),
    }
    if user_id:
        payload["user_id"] = user_id
    if chat_id:
        payload["chat_id"] = chat_id
    if use_care_mode:
        payload["care_mode"] = True
        if care_emotion:
            payload["care_emotion"] = care_emotion

    user_content = json.dumps(payload, ensure_ascii=False)

    return [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_content},
    ]

def _extract_text_from_message_obj(message: Any) -> str:
    """å…¼å®¹å¤šç¨® OpenAI Chat å›å‚³çµæ§‹ï¼Œç›¡å¯èƒ½æå–æ–‡å­—å…§å®¹ã€‚

    è¦†è“‹æƒ…æ³ï¼š
    - message.content ç‚ºå­—ä¸²
    - message.content ç‚ºå¤šæ®µé™£åˆ—ï¼ˆtype=text/image_url/...ï¼‰â†’ æ‹¼æ¥ text æ®µ
    - tool_calls / function_call â†’ å›å‚³ç°¡çŸ­ç³»çµ±æç¤ºæ–‡å­—
    - dict å½¢æ…‹çš„ message
    è‹¥ä»ç„¡å…§å®¹ï¼Œå›ç©ºå­—ä¸²ï¼Œäº¤ç”±ä¸Šå±¤è™•ç†ã€‚
    """
    try:
        if message is None:
            return ""

        # content å¯èƒ½æ˜¯ str æˆ– listï¼ˆå¤šæ¨¡æ…‹ï¼‰
        content = None
        try:
            content = getattr(message, "content", None)
        except Exception:
            content = None
        if content is None and isinstance(message, dict):
            content = message.get("content")

        if isinstance(content, str) and content.strip():
            return content.strip()

        if isinstance(content, list):
            parts: List[str] = []
            for p in content:
                p_type = None
                p_text = None
                try:
                    p_type = getattr(p, "type", None)
                except Exception:
                    p_type = p.get("type") if isinstance(p, dict) else None
                if p_type == "text":
                    try:
                        p_text = getattr(p, "text", None)
                    except Exception:
                        p_text = p.get("text") if isinstance(p, dict) else None
                    if p_text:
                        parts.append(str(p_text))
            if parts:
                return "\n".join(parts).strip()

        # tool_calls / function_call æç¤º
        tool_calls = None
        try:
            tool_calls = getattr(message, "tool_calls", None)
        except Exception:
            tool_calls = None
        if tool_calls is None and isinstance(message, dict):
            tool_calls = message.get("tool_calls")
        if tool_calls:
            return "[ç³»çµ±æç¤º] å·²è™•ç†å…§éƒ¨å·¥å…·å‘¼å«ã€‚"

        function_call = None
        try:
            function_call = getattr(message, "function_call", None)
        except Exception:
            function_call = None
        if function_call:
            return "[ç³»çµ±æç¤º] å·²è™•ç†å‡½å¼å‘¼å«ã€‚"

        # dict å½¢æ…‹æœ€å¾Œå˜—è©¦
        if isinstance(message, dict):
            text = message.get("content")
            if isinstance(text, str) and text.strip():
                return text.strip()

        return ""
    except Exception:
        return ""

def initialize_openai():
    """åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯"""
    global client
    api_key = settings.OPENAI_API_KEY
    if not api_key:
        logger.error("OpenAI APIå¯†é‘°æœªè¨­ç½®ï¼Œè«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½®OPENAI_API_KEYç’°å¢ƒè®Šæ•¸")
        print("\nâŒ éŒ¯èª¤: OpenAI APIå¯†é‘°æœªè¨­ç½®ï¼è«‹åœ¨.envæ–‡ä»¶ä¸­è¨­ç½®OPENAI_API_KEY\n")
        return False
    try:
        logger.info("æ­£åœ¨åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯...")
        client = OpenAI(api_key=api_key)
        logger.info("OpenAI å®¢æˆ¶ç«¯åˆå§‹åŒ–å®Œæˆ")
        return True
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–OpenAIå®¢æˆ¶ç«¯å¤±æ•—: {e}")
        print(f"\nâŒ OpenAI APIé€£æ¥å¤±æ•—: {e}\n")
        return False

## å·²ç§»é™¤å…§éƒ¨æ¸¬è©¦å‡½å¼ test_openai_responseï¼Œé¿å…å¹²æ“¾æ­£å¼æµç¨‹

async def generate_response_async(
    messages: List[Dict[str, str]],
    model: str = "gpt-5-nano",
    *,
    strict_json: bool = False,
    response_format: Optional[Dict[str, Any]] = None,
    use_structured_outputs: bool = False,
    response_schema: Optional[Dict[str, Any]] = None,
    max_tokens: Optional[int] = None,
    reasoning_effort: Optional[str] = None,
    stream: bool = False,
    on_chunk: Optional[Any] = None,
) -> str:
    """
    ç”ŸæˆAIå›æ‡‰ï¼ˆç•°æ­¥ç‰ˆæœ¬ï¼Œæ”¯æ´ Streamingï¼‰

    åƒæ•¸:
        messages: å°è©±è¨Šæ¯åˆ—è¡¨
        model: æ¨¡å‹åç¨±
        strict_json: æ˜¯å¦ä½¿ç”¨èˆŠç‰ˆ JSON æ¨¡å¼
        response_format: èˆŠç‰ˆå›æ‡‰æ ¼å¼ï¼ˆå·²æ£„ç”¨ï¼Œä½¿ç”¨ use_structured_outputsï¼‰
        use_structured_outputs: æ˜¯å¦ä½¿ç”¨æ–°ç‰ˆ Structured Outputs
        response_schema: JSON Schemaï¼ˆç”¨æ–¼ Structured Outputsï¼‰
        max_tokens: æœ€å¤§ tokens æ•¸é‡ï¼ˆæ–°å¢ï¼Œé—œæ‡·æ¨¡å¼ç”¨ï¼‰
        stream: æ˜¯å¦å•Ÿç”¨ä¸²æµæ¨¡å¼ï¼ˆ2025 æœ€ä½³å¯¦è¸ï¼‰
        on_chunk: ä¸²æµ chunk å›èª¿å‡½æ•¸ï¼ˆasync callableï¼‰
    """
    if client is None and not initialize_openai():
        return "æŠ±æ­‰ï¼ŒAIæœå‹™æš«æ™‚ä¸å¯ç”¨ã€‚ç³»çµ±ç„¡æ³•é€£æ¥åˆ°OpenAIæœå‹™ã€‚"
    try:
        start_time = time.time()
        loop = asyncio.get_event_loop()
        # åŠ ä¸Šè«‹æ±‚è¶…æ™‚ä¿è­·
        request_kwargs = {
            "model": model,
            "messages": messages,
            "max_completion_tokens": max_tokens if max_tokens else 2000,  # é—œæ‡·æ¨¡å¼å¯è‡ªè¨‚ tokens
        }

        # åŠ å…¥ reasoning_effort æ§åˆ¶ï¼ˆGPT-5 ç³»åˆ—ï¼‰
        if reasoning_effort:
            request_kwargs["reasoning_effort"] = reasoning_effort
            logger.info(f"ğŸ§  è¨­å®š reasoning_effort: {reasoning_effort}")

        # å„ªå…ˆä½¿ç”¨ Structured Outputsï¼ˆ2025å¹´æœ€ä½³å¯¦è¸ï¼‰
        if use_structured_outputs and response_schema:
            logger.info("ğŸ”§ ä½¿ç”¨ Structured Outputs æ¨¡å¼")
            request_kwargs["response_format"] = {
                "type": "json_schema",
                "json_schema": {
                    "name": "response_schema",
                    "strict": True,
                    "schema": response_schema
                }
            }
        # é™ç´šï¼šä½¿ç”¨èˆŠç‰ˆ JSON Object æ¨¡å¼
        elif strict_json or response_format:
            effective_response_format = response_format
            if strict_json and effective_response_format is None:
                effective_response_format = {"type": "json_object"}
            
            if effective_response_format is not None:
                logger.info("âš™ï¸ ä½¿ç”¨èˆŠç‰ˆ JSON Object æ¨¡å¼")
                request_kwargs["response_format"] = effective_response_format

        # 2025 æœ€ä½³å¯¦è¸ï¼šæ”¯æ´ Streaming Responses
        if stream and on_chunk:
            request_kwargs["stream"] = True
            logger.info("ğŸŒŠ å•Ÿç”¨ Streaming æ¨¡å¼")

            # ä½¿ç”¨ run_in_executor è™•ç†åŒæ­¥çš„ streaming API
            full_response = ""
            stream_obj = await loop.run_in_executor(
                None,
                lambda: client.chat.completions.create(**request_kwargs)
            )

            # é€å¡Šè™•ç†
            for chunk in stream_obj:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        full_response += delta.content
                        # ç•°æ­¥å›èª¿
                        if asyncio.iscoroutinefunction(on_chunk):
                            await on_chunk(delta.content)
                        else:
                            on_chunk(delta.content)

            ai_response = full_response
            logger.info(f"ğŸŒŠ Streaming å®Œæˆï¼Œç¸½é•·åº¦: {len(full_response)}")

        else:
            # éä¸²æµæ¨¡å¼ï¼ˆåŸé‚è¼¯ï¼‰
            response = await asyncio.wait_for(
                loop.run_in_executor(
                    None,
                    lambda: client.chat.completions.create(**request_kwargs),
                ),
                timeout=OPENAI_TIMEOUT,
            )
            try:
                logger.info(f"OpenAIå›å‚³æ¨¡å‹(async): {getattr(response, 'model', model)}")
            except Exception:
                pass
            # å…¼å®¹ä¸åŒå›å‚³çµæ§‹ï¼Œç¢ºä¿ä¸€å®šå›å­—ä¸²
            msg_obj = None
            try:
                msg_obj = response.choices[0].message
                logger.info(f"ğŸ“© GPT message ç‰©ä»¶: {msg_obj}")
            except Exception as e:
                logger.error(f"âŒ ç„¡æ³•å–å¾— response.choices[0].message: {e}")
                msg_obj = None

            ai_response = _extract_text_from_message_obj(msg_obj)
            logger.info(f"ğŸ“¤ æå–å¾Œçš„ ai_response: '{ai_response}' (é•·åº¦: {len(ai_response) if ai_response else 0})")

            if not ai_response:
                # æœ€å¾Œå˜—è©¦ç›´æ¥å– content æ¬„ä½ï¼ˆä¿åº•ï¼‰
                try:
                    raw = getattr(msg_obj, 'content', None)
                    logger.info(f"ğŸ“‹ msg_obj.content åŸå§‹å€¼: '{raw}' (type: {type(raw).__name__})")
                    if isinstance(raw, str):
                        ai_response = raw.strip()
                        logger.info(f"âœ… å¾ content æ¬„ä½å–å¾—å›æ‡‰: '{ai_response}'")
                except Exception as e:
                    logger.error(f"âŒ ç„¡æ³•å–å¾— msg_obj.content: {e}")

        if strict_json:
            normalized = (ai_response or "").strip()
            if not normalized:
                raise StrictResponseError("EMPTY_RESPONSE")
            try:
                json.loads(normalized)
            except json.JSONDecodeError as e:
                raise StrictResponseError("NON_JSON_RESPONSE", response=normalized) from e
            ai_response = normalized
        elif not ai_response:
            # æœ€çµ‚å…œåº•ï¼Œä½†å…ˆè¨˜éŒ„è©³ç´°æ—¥èªŒä»¥ä¾¿æ’æŸ¥
            logger.error(f"âŒ GPT å›æ‡‰ç‚ºç©ºï¼åŸå§‹ response ç‰©ä»¶: {response}")
            logger.error(f"âŒ msg_obj å…§å®¹: {msg_obj}")
            logger.error(f"âŒ æç¤ºè©: {messages}")
            ai_response = "æŠ±æ­‰ï¼Œæˆ‘æš«æ™‚æ²’æœ‰åˆé©çš„å›æ‡‰ã€‚å¯ä»¥æ›å€‹èªªæ³•å†è©¦è©¦å—ï¼Ÿ"

        elapsed_time = time.time() - start_time
        logger.info(f"AIå›æ‡‰ç”Ÿæˆå®Œæˆï¼Œè€—æ™‚: {elapsed_time:.2f}ç§’ï¼Œå›æ‡‰é•·åº¦: {len(ai_response)} å­—å…ƒ")
        return ai_response
    except Exception as e:
        if isinstance(e, StrictResponseError):
            raise
        logger.error(f"ç”Ÿæˆå›æ‡‰æ™‚å‡ºéŒ¯: {str(e)}")
        error_message = str(e).lower()
        if isinstance(e, asyncio.TimeoutError):
            return "æŠ±æ­‰ï¼Œé€£æ¥AIæœå‹™è¶…æ™‚ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"
        if "api key" in error_message or "authentication" in error_message:
            return "æŠ±æ­‰ï¼ŒAIæœå‹™æš«æ™‚ä¸å¯ç”¨ã€‚è«‹æª¢æŸ¥APIå¯†é‘°è¨­ç½®ã€‚"
        elif "timeout" in error_message or "connection" in error_message:
            return "æŠ±æ­‰ï¼Œé€£æ¥AIæœå‹™è¶…æ™‚ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"
        elif "rate limit" in error_message:
            return "æŠ±æ­‰ï¼ŒAIæœå‹™æš«æ™‚é”åˆ°è«‹æ±‚é™åˆ¶ã€‚è«‹ç¨å¾Œå†è©¦ã€‚"
        elif "model" in error_message and ("not found" in error_message or "does not exist" in error_message):
            return "æŠ±æ­‰ï¼Œè«‹æ±‚çš„AIæ¨¡å‹ä¸å¯ç”¨ã€‚"
        else:
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå›æ‡‰æ™‚é‡åˆ°å•é¡Œã€‚è«‹é‡è©¦ã€‚"

async def generate_response_for_user(
    user_message: str = None,
    user_id: str = "default",
    messages: List[Dict[str, str]] = None,
    model: str = "gpt-5-nano",
    request_id: Optional[str] = None,
    chat_id: Optional[str] = None,
    *,
    strict_json: bool = False,
    response_format: Optional[Dict[str, Any]] = None,
    use_structured_outputs: bool = False,
    response_schema: Optional[Dict[str, Any]] = None,
    use_care_mode: bool = False,
    care_emotion: Optional[str] = None,
    reasoning_effort: Optional[str] = None,
    user_name: Optional[str] = None,
) -> str:
    """
    ç‚ºç”¨æˆ¶ç”ŸæˆAIå›æ‡‰

    åƒæ•¸:
        use_structured_outputs: æ˜¯å¦ä½¿ç”¨ Structured Outputsï¼ˆ2025å¹´æœ€ä½³å¯¦è¸ï¼‰
        response_schema: JSON Schemaï¼ˆé…åˆ Structured Outputs ä½¿ç”¨ï¼‰
        use_care_mode: æ˜¯å¦ä½¿ç”¨æƒ…ç·’é—œæ‡·æ¨¡å¼ï¼ˆæ–°å¢ï¼‰
        care_emotion: é—œæ‡·æ¨¡å¼çš„æƒ…ç·’æ¨™ç±¤ï¼ˆæ–°å¢ï¼‰
        reasoning_effort: æ¨ç†å¼·åº¦ (minimal/low/medium/high)ï¼Œç”¨æ–¼æ§åˆ¶ reasoning tokens
    """
    logger.info(f"ç”Ÿæˆå›æ‡‰è«‹æ±‚ï¼Œä½¿ç”¨æ¨¡å‹: {model} req_id={request_id} chat_id={chat_id} structured={use_structured_outputs}")
    try:
        # å¦‚æœæä¾›äº†chat_idï¼Œä½¿ç”¨DBç®¡ç†å°è©±æ­·å²
        if chat_id and db_available:
            return await _generate_response_with_chat_db(
                user_message,
                user_id,
                messages,
                model,
                chat_id,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
                reasoning_effort=reasoning_effort,
                user_name=user_name,
            )
        else:
            # å›é€€åˆ°åŸæœ‰çš„å…¨å±€æ­·å²ç®¡ç†ï¼ˆç”¨æ–¼å‘å¾Œå…¼å®¹ï¼‰
            return await _generate_response_with_global_history(
                user_message,
                user_id,
                messages,
                model,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
                reasoning_effort=reasoning_effort,
                user_name=user_name,
            )

        logger.error("æœªæä¾›æ¶ˆæ¯åˆ—è¡¨æˆ–ç”¨æˆ¶æ¶ˆæ¯")
        return "æŠ±æ­‰ï¼Œæ²’æœ‰æ”¶åˆ°è™•ç†è«‹æ±‚æ‰€éœ€çš„æ¶ˆæ¯å…§å®¹ã€‚"
    except StrictResponseError:
        raise
    except Exception as e:
        logger.error(f"ç”Ÿæˆå›æ‡‰æ™‚å‡ºéŒ¯: {str(e)}")
        return f"æŠ±æ­‰ï¼Œæˆ‘ç¾åœ¨ç„¡æ³•æä¾›å›æ‡‰ã€‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"


async def _generate_response_with_chat_db(
    user_message,
    user_id,
    messages,
    model,
    chat_id,
    *,
    strict_json: bool = False,
    response_format: Optional[Dict[str, Any]] = None,
    use_structured_outputs: bool = False,
    response_schema: Optional[Dict[str, Any]] = None,
    use_care_mode: bool = False,
    care_emotion: Optional[str] = None,
    reasoning_effort: Optional[str] = None,
    user_name: Optional[str] = None,
):
    """ä½¿ç”¨DBç®¡ç†å°è©±æ­·å²çš„å¯¦ç¾"""
    try:
        if messages:
            if not any(msg.get("role") == "system" for msg in messages):
                # æ ¹æ“šæ˜¯å¦ç‚ºé—œæ‡·æ¨¡å¼é¸æ“‡ System Promptï¼ˆæ–°å¢ï¼‰
                if use_care_mode:
                    emotion_text = f"ï¼ˆç”¨æˆ¶æƒ…ç·’ï¼š{care_emotion}ï¼‰" if care_emotion else ""
                    system_prompt = f"{CARE_MODE_SYSTEM_PROMPT}\n\n{emotion_text}"
                    logger.info(f"ğŸ’™ ä½¿ç”¨é—œæ‡·æ¨¡å¼ System Promptï¼Œæƒ…ç·’ï¼š{care_emotion}")
                else:
                    system_prompt = "ä½ æ˜¯ä¸€å€‹å‹å–„ã€æœ‰ç¦®ã€å¹½é»˜ä¸”èƒ½å¤ æä¾›å¹«åŠ©çš„AIåŠ©æ‰‹ã€‚è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›è¦†ï¼Œä¿æŒç°¡æ½”æ¸…æ™°çš„è¡¨é”ã€‚"

                # åœ¨ç³»çµ±æç¤ºå‰åŠ ä¸Šç”¨æˆ¶åç¨±
                if user_name:
                    system_prompt = f"ç”¨æˆ¶åç¨±ï¼š{user_name}\n\n{system_prompt}"

                messages.insert(0, {"role": "system", "content": system_prompt})
            ai_response = await generate_response_async(
                messages,
                model=model,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                max_tokens=2000 if use_care_mode else None,  # é—œæ‡·æ¨¡å¼ 2000 tokensï¼ˆgpt-5-nano reasoning + å¯¦éš›è¼¸å‡ºï¼‰
                reasoning_effort=reasoning_effort or ("minimal" if use_care_mode else "low"),  # 2025 æœ€ä½³å¯¦è¸ï¼šé—œæ‡·æ¨¡å¼ minimalï¼Œä¸€èˆ¬å°è©± low
            )
            # ä¿å­˜AIå›æ‡‰åˆ°DB
            if db_available:
                try:
                    await save_chat_message(chat_id, "assistant", ai_response)
                except Exception as e:
                    logger.warning(f"ä¿å­˜AIå›æ‡‰åˆ°DBå¤±æ•—: {e}")
            return ai_response

        if user_message:
            # ä¿å­˜ç”¨æˆ¶æ¶ˆæ¯åˆ°DB
            if db_available:
                try:
                    await save_chat_message(chat_id, "user", user_message)
                except Exception as e:
                    logger.warning(f"ä¿å­˜ç”¨æˆ¶æ¶ˆæ¯åˆ°DBå¤±æ•—: {e}")

            # å¾DBåŠ è¼‰å°è©±æ­·å²
            chat_history = []
            if db_available:
                try:
                    chat_result = await get_chat(chat_id)
                    if chat_result.get("success"):
                        chat_messages = chat_result["chat"].get("messages", [])

                        # é—œæ‡·æ¨¡å¼åªè¼‰å…¥æœ€è¿‘ 5 æ¢ï¼Œä¸€èˆ¬æ¨¡å¼è¼‰å…¥ 10 æ¢ï¼ˆæ¸›å°‘ä¸Šä¸‹æ–‡ï¼‰
                        history_limit = 5 if use_care_mode else 10

                        # âš ï¸ é—œéµä¿®å¾©ï¼šæ’é™¤ç•¶å‰ç”¨æˆ¶è¨Šæ¯ï¼ˆé¿å… Agent æ··æ·†æ­·å²å°è©±ï¼‰
                        # åªè¼‰å…¥æ­·å²å°è©±ï¼Œä¸åŒ…å«å‰›ä¿å­˜çš„ user_message
                        historical_messages = chat_messages[:-1] if len(chat_messages) > 0 else []

                        # è½‰æ›DBæ ¼å¼åˆ°OpenAIæ ¼å¼
                        for msg in historical_messages[-history_limit:]:
                            content = msg.get("content")
                            # ç¢ºä¿ content æ˜¯å­—ä¸²ï¼ˆä¿®æ­£ï¼‰
                            if isinstance(content, dict):
                                content = content.get("message") or content.get("text") or str(content)
                            elif not isinstance(content, str):
                                content = str(content) if content else ""

                            # éæ¿¾æ‰éŒ¯èª¤è¨Šæ¯ï¼ˆé¿å…æ±¡æŸ“ä¸Šä¸‹æ–‡ï¼‰
                            if "æŠ±æ­‰ï¼Œç”Ÿæˆå›æ‡‰æ™‚é‡åˆ°å•é¡Œ" in content or "è«‹é‡è©¦" in content:
                                continue

                            chat_history.append({
                                "role": msg.get("sender"),
                                "content": content
                            })

                        logger.debug(f"ğŸ“š è¼‰å…¥ {len(chat_history)} æ¢æ­·å²å°è©±ï¼ˆæ’é™¤ç•¶å‰è¨Šæ¯ï¼Œç¢ºä¿è«‹æ±‚éš”é›¢ï¼‰")
                except Exception as e:
                    logger.warning(f"å¾DBåŠ è¼‰å°è©±æ­·å²å¤±æ•—: {e}")

            # è¼‰å…¥é•·æœŸè¨˜æ†¶
            memory_context = ""
            if user_id:
                try:
                    from core.memory_system import memory_system
                    context_tags: List[str] = []
                    if use_care_mode:
                        context_tags.append("care_mode")
                    if care_emotion:
                        context_tags.append(str(care_emotion))
                    relevant_memories = await memory_system.get_relevant_memories(
                        user_id=user_id,
                        current_message=user_message,
                        max_memories=5,
                        context_tags=context_tags or None,
                    )
                    if relevant_memories:
                        memory_context = memory_system.format_memories_for_context(relevant_memories)
                        logger.info(f"ğŸ“š è¼‰å…¥ {len(relevant_memories)} æ¢ç›¸é—œè¨˜æ†¶")
                except Exception as e:
                    logger.warning(f"è¼‰å…¥è¨˜æ†¶å¤±æ•—: {e}")

            base_prompt = _build_base_system_prompt(
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
                user_name=user_name,
            )

            messages_to_send = _compose_messages_with_context(
                base_prompt=base_prompt,
                history_entries=chat_history,
                memory_context=memory_context,
                current_request=user_message,
                user_id=user_id,
                chat_id=chat_id,
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
            )
            ai_response = await generate_response_async(
                messages_to_send,
                model=model,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                max_tokens=2000 if use_care_mode else None,  # é—œæ‡·æ¨¡å¼ 2000 tokensï¼ˆgpt-5-nano reasoning + å¯¦éš›è¼¸å‡ºï¼‰
                reasoning_effort=reasoning_effort or ("minimal" if use_care_mode else "low"),  # 2025 æœ€ä½³å¯¦è¸ï¼šé—œæ‡·æ¨¡å¼ minimalï¼Œä¸€èˆ¬å°è©± low
            )

            # ä¿å­˜AIå›æ‡‰åˆ°DB
            if db_available:
                try:
                    await save_chat_message(chat_id, "assistant", ai_response)
                except Exception as e:
                    logger.warning(f"ä¿å­˜AIå›æ‡‰åˆ°DBå¤±æ•—: {e}")

            return ai_response

    except Exception as e:
        if isinstance(e, StrictResponseError):
            raise
        logger.error(f"DBå°è©±è™•ç†å‡ºéŒ¯: {e}")
        # å›é€€åˆ°å…¨å±€æ­·å²
        return await _generate_response_with_global_history(
            user_message,
            user_id,
            messages,
            model,
            strict_json=strict_json,
            response_format=response_format,
            use_structured_outputs=use_structured_outputs,
            response_schema=response_schema,
            use_care_mode=use_care_mode,
            care_emotion=care_emotion,
            reasoning_effort=reasoning_effort,
            user_name=user_name,
        )


async def _generate_response_with_global_history(
    user_message,
    user_id,
    messages,
    model,
    *,
    strict_json: bool = False,
    response_format: Optional[Dict[str, Any]] = None,
    use_structured_outputs: bool = False,
    response_schema: Optional[Dict[str, Any]] = None,
    use_care_mode: bool = False,
    care_emotion: Optional[str] = None,
    reasoning_effort: Optional[str] = None,
    user_name: Optional[str] = None,
):
    """ä½¿ç”¨å…¨å±€æ­·å²çš„å›é€€å¯¦ç¾ï¼ˆå‘å¾Œå…¼å®¹ï¼‰"""
    try:
        if messages:
            if not any(msg.get("role") == "system" for msg in messages):
                # æ ¹æ“šæ˜¯å¦ç‚ºé—œæ‡·æ¨¡å¼é¸æ“‡ System Promptï¼ˆæ–°å¢ï¼‰
                if use_care_mode:
                    emotion_text = f"ï¼ˆç”¨æˆ¶æƒ…ç·’ï¼š{care_emotion}ï¼‰" if care_emotion else ""
                    system_prompt = f"{CARE_MODE_SYSTEM_PROMPT}\n\n{emotion_text}"
                    logger.info(f"ğŸ’™ ä½¿ç”¨é—œæ‡·æ¨¡å¼ System Promptï¼ˆå…¨å±€æ­·å²ï¼‰ï¼Œæƒ…ç·’ï¼š{care_emotion}")
                else:
                    system_prompt = "ä½ æ˜¯ä¸€å€‹å‹å–„ã€æœ‰ç¦®ã€å¹½é»˜ä¸”èƒ½å¤ æä¾›å¹«åŠ©çš„AIåŠ©æ‰‹ã€‚è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›è¦†ï¼Œä¿æŒç°¡æ½”æ¸…æ™°çš„è¡¨é”ã€‚"

                # åœ¨ç³»çµ±æç¤ºå‰åŠ ä¸Šç”¨æˆ¶åç¨±
                if user_name:
                    system_prompt = f"ç”¨æˆ¶åç¨±ï¼š{user_name}\n\n{system_prompt}"

                messages.insert(0, {"role": "system", "content": system_prompt})
            user_messages = [msg for msg in messages if msg.get("role") == "user"]
            if user_messages and user_id not in conversation_history:
                conversation_history[user_id] = []
                conversation_history[user_id].extend(user_messages[-5:])
            ai_response = await generate_response_async(
                messages,
                model=model,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                max_tokens=2000 if use_care_mode else None,  # é—œæ‡·æ¨¡å¼ 2000 tokensï¼ˆgpt-5-nano reasoning + å¯¦éš›è¼¸å‡ºï¼‰
                reasoning_effort=reasoning_effort or ("minimal" if use_care_mode else "low"),  # 2025 æœ€ä½³å¯¦è¸ï¼šé—œæ‡·æ¨¡å¼ minimalï¼Œä¸€èˆ¬å°è©± low
            )
            if user_id in conversation_history:
                conversation_history[user_id].append({"role": "assistant", "content": ai_response})
                if len(conversation_history[user_id]) > 50:
                    conversation_history[user_id] = conversation_history[user_id][-50:]
            return ai_response

        if user_message:
            if user_id not in conversation_history:
                conversation_history[user_id] = []
            conversation_history[user_id].append({"role": "user", "content": user_message})

            history_limit = 5 if use_care_mode else 10
            prior_history = conversation_history[user_id][:-1]
            if prior_history:
                prior_history = prior_history[-history_limit:]

            base_prompt = _build_base_system_prompt(
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
                user_name=user_name,
            )

            memory_context = ""
            if user_id:
                try:
                    from core.memory_system import memory_system
                    context_tags: List[str] = []
                    if use_care_mode:
                        context_tags.append("care_mode")
                    if care_emotion:
                        context_tags.append(str(care_emotion))
                    relevant_memories = await memory_system.get_relevant_memories(
                        user_id=user_id,
                        current_message=user_message,
                        max_memories=5,
                        context_tags=context_tags or None,
                    )
                    if relevant_memories:
                        memory_context = memory_system.format_memories_for_context(relevant_memories)
                except Exception as ex:
                    logger.warning(f"è¼‰å…¥å…¨å±€è¨˜æ†¶å¤±æ•—: {ex}")

            messages_to_send = _compose_messages_with_context(
                base_prompt=base_prompt,
                history_entries=prior_history,
                memory_context=memory_context,
                current_request=user_message,
                user_id=user_id,
                chat_id=None,
                use_care_mode=use_care_mode,
                care_emotion=care_emotion,
            )
            ai_response = await generate_response_async(
                messages_to_send,
                model=model,
                strict_json=strict_json,
                response_format=response_format,
                use_structured_outputs=use_structured_outputs,
                response_schema=response_schema,
                max_tokens=2000 if use_care_mode else None,  # é—œæ‡·æ¨¡å¼ 2000 tokensï¼ˆgpt-5-nano reasoning + å¯¦éš›è¼¸å‡ºï¼‰
                reasoning_effort=reasoning_effort or ("minimal" if use_care_mode else "low"),  # 2025 æœ€ä½³å¯¦è¸ï¼šé—œæ‡·æ¨¡å¼ minimalï¼Œä¸€èˆ¬å°è©± low
            )
            conversation_history[user_id].append({"role": "assistant", "content": ai_response})
            if len(conversation_history[user_id]) > 50:
                conversation_history[user_id] = conversation_history[user_id][-50:]
            return ai_response

    except Exception as e:
        if isinstance(e, StrictResponseError):
            raise
        logger.error(f"å…¨å±€æ­·å²è™•ç†å‡ºéŒ¯: {e}")
        raise

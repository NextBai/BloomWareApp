"""
OpenAI Batch API è™•ç†å™¨ï¼ˆ2025 æœ€ä½³å¯¦è¸ï¼‰
ç”¨æ–¼éå³æ™‚ä»»å‹™ï¼Œæˆæœ¬é™ä½ 50%

é©ç”¨å ´æ™¯ï¼š
- é•·æœŸè¨˜æ†¶æ‘˜è¦ï¼ˆæ¯æ—¥å‡Œæ™¨æ‰¹æ¬¡è™•ç†ï¼‰
- å¥åº·æ•¸æ“šåˆ†æå ±å‘Š
- æƒ…ç·’åˆ†æé€±å ±
- å¤§é‡æ–‡å­—ç¿»è­¯/æ‘˜è¦

åƒè€ƒï¼šhttps://cookbook.openai.com/examples/batch_processing
"""

import os
import json
import time
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
from pathlib import Path
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger("batch_processor")

# OpenAI å®¢æˆ¶ç«¯
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Batch æª”æ¡ˆå„²å­˜ç›®éŒ„
BATCH_DIR = Path("/tmp/openai_batch")
BATCH_DIR.mkdir(exist_ok=True, parents=True)


class BatchProcessor:
    """
    OpenAI Batch API è™•ç†å™¨

    ä½¿ç”¨æ–¹å¼ï¼š
        processor = BatchProcessor()
        batch_id = await processor.create_memory_summary_batch(user_ids)
        result = await processor.wait_for_completion(batch_id)
    """

    def __init__(self):
        self.client = client

    def create_batch_request(
        self,
        custom_id: str,
        model: str,
        messages: List[Dict[str, str]],
        **kwargs
    ) -> Dict[str, Any]:
        """
        å‰µå»ºå–®å€‹æ‰¹æ¬¡è«‹æ±‚ï¼ˆJSONL æ ¼å¼ï¼‰

        Args:
            custom_id: è‡ªè¨‚ IDï¼ˆç”¨æ–¼è­˜åˆ¥çµæœï¼‰
            model: æ¨¡å‹åç¨±
            messages: å°è©±è¨Šæ¯
            **kwargs: å…¶ä»–åƒæ•¸ï¼ˆå¦‚ max_tokens, temperatureï¼‰

        Returns:
            JSONL æ ¼å¼çš„è«‹æ±‚ç‰©ä»¶
        """
        return {
            "custom_id": custom_id,
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": model,
                "messages": messages,
                **kwargs
            }
        }

    async def create_batch_file(
        self,
        requests: List[Dict[str, Any]],
        filename: Optional[str] = None
    ) -> str:
        """
        å‰µå»ºæ‰¹æ¬¡æª”æ¡ˆï¼ˆJSONL æ ¼å¼ï¼‰

        Args:
            requests: æ‰¹æ¬¡è«‹æ±‚åˆ—è¡¨
            filename: æª”æ¡ˆåç¨±ï¼ˆå¯é¸ï¼‰

        Returns:
            æ‰¹æ¬¡æª”æ¡ˆè·¯å¾‘
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"batch_{timestamp}.jsonl"

        file_path = BATCH_DIR / filename

        # å¯«å…¥ JSONL æ ¼å¼
        with open(file_path, "w", encoding="utf-8") as f:
            for req in requests:
                f.write(json.dumps(req, ensure_ascii=False) + "\n")

        logger.info(f"âœ… æ‰¹æ¬¡æª”æ¡ˆå·²å‰µå»º: {file_path}ï¼ˆ{len(requests)} å€‹è«‹æ±‚ï¼‰")
        return str(file_path)

    async def submit_batch(
        self,
        file_path: str,
        description: Optional[str] = None
    ) -> str:
        """
        æäº¤æ‰¹æ¬¡ä»»å‹™åˆ° OpenAI

        Args:
            file_path: æ‰¹æ¬¡æª”æ¡ˆè·¯å¾‘
            description: æ‰¹æ¬¡æè¿°ï¼ˆå¯é¸ï¼‰

        Returns:
            batch_id
        """
        # ä¸Šå‚³æª”æ¡ˆ
        with open(file_path, "rb") as f:
            batch_file = self.client.files.create(
                file=f,
                purpose="batch"
            )

        logger.info(f"ğŸ“¤ æª”æ¡ˆå·²ä¸Šå‚³: {batch_file.id}")

        # æäº¤æ‰¹æ¬¡ä»»å‹™
        batch_job = self.client.batches.create(
            input_file_id=batch_file.id,
            endpoint="/v1/chat/completions",
            completion_window="24h",
            metadata={"description": description} if description else None
        )

        logger.info(f"ğŸš€ æ‰¹æ¬¡ä»»å‹™å·²æäº¤: {batch_job.id}")
        logger.info(f"ğŸ“Š ç‹€æ…‹: {batch_job.status}")

        return batch_job.id

    async def wait_for_completion(
        self,
        batch_id: str,
        poll_interval: int = 60,
        max_wait_time: int = 86400  # 24å°æ™‚
    ) -> Dict[str, Any]:
        """
        ç­‰å¾…æ‰¹æ¬¡ä»»å‹™å®Œæˆï¼ˆéé˜»å¡ï¼‰

        Args:
            batch_id: æ‰¹æ¬¡ ID
            poll_interval: è¼ªè©¢é–“éš”ï¼ˆç§’ï¼‰ï¼Œé è¨­ 60 ç§’
            max_wait_time: æœ€å¤§ç­‰å¾…æ™‚é–“ï¼ˆç§’ï¼‰ï¼Œé è¨­ 24 å°æ™‚

        Returns:
            æ‰¹æ¬¡çµæœ
        """
        start_time = time.time()

        while True:
            # æª¢æŸ¥æ˜¯å¦è¶…æ™‚
            if time.time() - start_time > max_wait_time:
                logger.error(f"âŒ æ‰¹æ¬¡ä»»å‹™ {batch_id} è¶…æ™‚ï¼ˆ{max_wait_time}ç§’ï¼‰")
                raise TimeoutError(f"Batch {batch_id} timeout after {max_wait_time}s")

            # æŸ¥è©¢æ‰¹æ¬¡ç‹€æ…‹
            batch_job = self.client.batches.retrieve(batch_id)

            logger.info(f"ğŸ“Š æ‰¹æ¬¡ {batch_id} ç‹€æ…‹: {batch_job.status}")

            if batch_job.status == "completed":
                logger.info(f"âœ… æ‰¹æ¬¡ä»»å‹™å®Œæˆ: {batch_id}")
                return await self._retrieve_results(batch_job)

            elif batch_job.status == "failed":
                logger.error(f"âŒ æ‰¹æ¬¡ä»»å‹™å¤±æ•—: {batch_id}")
                return {
                    "success": False,
                    "error": "Batch job failed",
                    "batch_id": batch_id
                }

            elif batch_job.status == "cancelled":
                logger.warning(f"âš ï¸ æ‰¹æ¬¡ä»»å‹™å·²å–æ¶ˆ: {batch_id}")
                return {
                    "success": False,
                    "error": "Batch job cancelled",
                    "batch_id": batch_id
                }

            # ç­‰å¾…ä¸‹ä¸€æ¬¡è¼ªè©¢
            await asyncio.sleep(poll_interval)

    async def _retrieve_results(self, batch_job: Any) -> Dict[str, Any]:
        """
        æå–æ‰¹æ¬¡çµæœ

        Args:
            batch_job: æ‰¹æ¬¡ä»»å‹™ç‰©ä»¶

        Returns:
            è§£æå¾Œçš„çµæœ
        """
        # ä¸‹è¼‰çµæœæª”æ¡ˆ
        result_file_id = batch_job.output_file_id
        result_content = self.client.files.content(result_file_id)

        # è§£æ JSONL çµæœ
        results = []
        for line in result_content.text.strip().split("\n"):
            if line:
                results.append(json.loads(line))

        logger.info(f"ğŸ“¥ æ‰¹æ¬¡çµæœå·²ä¸‹è¼‰: {len(results)} å€‹å›æ‡‰")

        return {
            "success": True,
            "batch_id": batch_job.id,
            "total_requests": len(results),
            "results": results,
            "metadata": {
                "created_at": batch_job.created_at,
                "completed_at": batch_job.completed_at,
                "request_counts": batch_job.request_counts
            }
        }

    # ========== å…·é«”æ‡‰ç”¨å ´æ™¯ ==========

    async def create_memory_summary_batch(
        self,
        user_memories: Dict[str, List[str]],
        model: str = "gpt-5-nano"
    ) -> str:
        """
        å‰µå»ºè¨˜æ†¶æ‘˜è¦æ‰¹æ¬¡ä»»å‹™

        Args:
            user_memories: {user_id: [memory_1, memory_2, ...]}
            model: æ¨¡å‹åç¨±

        Returns:
            batch_id
        """
        requests = []

        for user_id, memories in user_memories.items():
            # çµ„è£æç¤ºè©
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯è¨˜æ†¶æ‘˜è¦åŠ©æ‰‹ï¼Œè«‹å°‡ç”¨æˆ¶çš„å¤šæ¢è¨˜æ†¶æ•´åˆç‚ºç°¡æ½”çš„æ‘˜è¦ã€‚"
                },
                {
                    "role": "user",
                    "content": f"è«‹æ‘˜è¦ä»¥ä¸‹è¨˜æ†¶ï¼š\n" + "\n".join(f"- {m}" for m in memories)
                }
            ]

            # å‰µå»ºè«‹æ±‚
            req = self.create_batch_request(
                custom_id=user_id,
                model=model,
                messages=messages,
                max_tokens=500,
                reasoning_effort="medium"  # æ‰¹æ¬¡ä»»å‹™å¯ç”¨è¼ƒé«˜æ¨ç†å¼·åº¦
            )

            requests.append(req)

        # å‰µå»ºæ‰¹æ¬¡æª”æ¡ˆ
        file_path = await self.create_batch_file(requests, filename="memory_summary.jsonl")

        # æäº¤æ‰¹æ¬¡
        batch_id = await self.submit_batch(file_path, description="æ¯æ—¥è¨˜æ†¶æ‘˜è¦")

        return batch_id

    async def create_health_report_batch(
        self,
        user_health_data: Dict[str, Dict[str, Any]],
        model: str = "gpt-5-nano"
    ) -> str:
        """
        å‰µå»ºå¥åº·å ±å‘Šæ‰¹æ¬¡ä»»å‹™

        Args:
            user_health_data: {user_id: {heart_rate: ..., steps: ...}}
            model: æ¨¡å‹åç¨±

        Returns:
            batch_id
        """
        requests = []

        for user_id, health_data in user_health_data.items():
            messages = [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯å¥åº·åˆ†æåŠ©æ‰‹ï¼Œè«‹æ ¹æ“šç”¨æˆ¶çš„å¥åº·æ•¸æ“šç”Ÿæˆé€±å ±ã€‚"
                },
                {
                    "role": "user",
                    "content": f"è«‹åˆ†æä»¥ä¸‹å¥åº·æ•¸æ“šä¸¦ç”Ÿæˆå ±å‘Šï¼š\n{json.dumps(health_data, ensure_ascii=False, indent=2)}"
                }
            ]

            req = self.create_batch_request(
                custom_id=user_id,
                model=model,
                messages=messages,
                max_tokens=1000,
                reasoning_effort="medium"
            )

            requests.append(req)

        file_path = await self.create_batch_file(requests, filename="health_report.jsonl")
        batch_id = await self.submit_batch(file_path, description="å¥åº·é€±å ±")

        return batch_id


# å…¨åŸŸå–®ä¾‹
batch_processor = BatchProcessor()


# ========== ä¾¿æ·å‡½æ•¸ ==========

async def submit_memory_summary_batch(user_memories: Dict[str, List[str]]) -> str:
    """
    ä¾¿æ·å‡½æ•¸ï¼šæäº¤è¨˜æ†¶æ‘˜è¦æ‰¹æ¬¡ä»»å‹™

    ç¯„ä¾‹ï¼š
        user_memories = {
            "user_123": ["ä»Šå¤©å»äº†å…¬åœ’", "åƒäº†ç¾©å¤§åˆ©éºµ", "å¿ƒæƒ…ä¸éŒ¯"],
            "user_456": ["å·¥ä½œå¾ˆå¿™", "æ™šä¸Šå¥èº«"],
        }
        batch_id = await submit_memory_summary_batch(user_memories)
    """
    return await batch_processor.create_memory_summary_batch(user_memories)


async def get_batch_results(batch_id: str) -> Dict[str, Any]:
    """
    ä¾¿æ·å‡½æ•¸ï¼šç²å–æ‰¹æ¬¡çµæœï¼ˆç­‰å¾…å®Œæˆï¼‰

    ç¯„ä¾‹ï¼š
        results = await get_batch_results(batch_id)
        if results["success"]:
            for item in results["results"]:
                logger.debug(item)
    """
    return await batch_processor.wait_for_completion(batch_id)
